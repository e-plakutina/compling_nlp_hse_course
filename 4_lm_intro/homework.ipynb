{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00fad453",
      "metadata": {
        "id": "00fad453"
      },
      "source": [
        "# Домашнее задание № 4. Языковые модели"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d056af4",
      "metadata": {
        "id": "5d056af4"
      },
      "source": [
        "## Задание 1 (8 баллов)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f532a8",
      "metadata": {
        "id": "d1f532a8"
      },
      "source": [
        "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de743d1d",
      "metadata": {
        "id": "de743d1d"
      },
      "source": [
        "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
        "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели.\n",
        "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
        "\n",
        "\n",
        "Подсказки:  \n",
        "    - нужно будет добавить еще один тэг \\<start>  \n",
        "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хронить биграмы, а по колонкам униграммы\n",
        "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
        "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d078056d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d078056d",
        "outputId": "393a0d4c-56e6-49b9-d4f4-7f723d173bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install razdel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6afcef88",
      "metadata": {
        "id": "6afcef88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af32255d-90f4-4bb2-8887-d02ec2e07240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "from string import punctuation\n",
        "from razdel import sentenize\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from scipy.sparse import lil_matrix, csr_matrix, csc_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus = open('2ch_corpus.txt').read()"
      ],
      "metadata": {
        "id": "I57aDaso6XZY"
      },
      "id": "I57aDaso6XZY",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = open('lenta.txt').read()"
      ],
      "metadata": {
        "id": "zmDyxRZ5Al17"
      },
      "id": "zmDyxRZ5Al17",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# удаляем пунктуацию + приводим к нижнему регистру\n",
        "\n",
        "def normalize(text):\n",
        "    normalized_text = [word.text.strip(punctuation) for word \\\n",
        "                                                            in razdel_tokenize(text)]\n",
        "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
        "    return normalized_text"
      ],
      "metadata": {
        "id": "mzTD89a_6b43"
      },
      "id": "mzTD89a_6b43",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# получаем n-граммы\n",
        "\n",
        "def ngrammer(tokens, n=2):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "tDbM5cqh7ZQZ"
      },
      "id": "tDbM5cqh7ZQZ",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# получаем предложения из корпуса, в которых удалена пунктуация и слова приведены к нижнему регистру\n",
        "\n",
        "# еще у нас есть служебные токены, которые сигнализируют начало и конец предложения\n",
        "\n",
        "# <start> 2 раза, чтобы у нас появлялись триграммы (когда предложение начинается, у него нет двух предыдущих слов)\n",
        "\n",
        "sentences_corpus = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(corpus[:5000000])]"
      ],
      "metadata": {
        "id": "2pUJVMjG9O73"
      },
      "id": "2pUJVMjG9O73",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделяем данные: 90% для обучения, 10% для проверки\n",
        "training_sentences = sentences_corpus[:int(0.9 * len(sentences_corpus))]\n",
        "held_out_sentences = sentences_corpus[int(0.9 * len(sentences_corpus)):]"
      ],
      "metadata": {
        "id": "LrxqilxrcZiL"
      },
      "id": "LrxqilxrcZiL",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams_corpus = Counter()\n",
        "bigrams_corpus = Counter()\n",
        "trigrams_corpus = Counter()\n",
        "\n",
        "for sentence in training_sentences:\n",
        "    unigrams_corpus.update(sentence)\n",
        "    bigrams_corpus.update(ngrammer(sentence, n=2))\n",
        "    trigrams_corpus.update(ngrammer(sentence, n=3))"
      ],
      "metadata": {
        "id": "tyQdrgzV9PmK"
      },
      "id": "tyQdrgzV9PmK",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# матрица слова на слова (инициализируем нулями)\n",
        "matrix_corpus = lil_matrix((len(bigrams_corpus),\n",
        "                          len(unigrams_corpus)))\n",
        "\n",
        "# маппинг для слов (id-слово и слово-id)\n",
        "# к матрице нужно обращаться по индексам\n",
        "# поэтому зафиксируем порядок слов в словаре и сделаем маппинг id-слово и слово-id\n",
        "id2word_corpus = list(unigrams_corpus)\n",
        "word2id_corpus = {word:i for i, word in enumerate(id2word_corpus)}\n",
        "\n",
        "# маппинг для биграмм:\n",
        "bigram_keys = list(bigrams_corpus.keys())\n",
        "id2bigram_corpus = {i: bigram for i, bigram in enumerate(bigram_keys)}\n",
        "bigram2id_corpus = {bigram: i for i, bigram in enumerate(bigram_keys)}\n",
        "\n",
        "\n",
        "\n",
        "# Заполняем матрицу вероятностей для триграмм\n",
        "for trigram in trigrams_corpus:\n",
        "    word1, word2, word3 = trigram.split()\n",
        "    bigram = f\"{word1} {word2}\"\n",
        "    # На пересечение биграммы и слова ставим вероятность встретить третье слово после биграммы\n",
        "    matrix_corpus[bigram2id_corpus[bigram], word2id_corpus[word3]] = (\n",
        "        trigrams_corpus[trigram] / bigrams_corpus[bigram]\n",
        "    )\n",
        "\n",
        "\n",
        "matrix_corpus = csc_matrix(matrix_corpus)"
      ],
      "metadata": {
        "id": "AhHbrX_o9wD6"
      },
      "id": "AhHbrX_o9wD6",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(matrix, id2word, word2id, id2bigram, bigram2id, n=100, start='<start>'):\n",
        "    text = [start, start] # начало текста с двух токенов <start>\n",
        "    current_idx = bigram2id[f\"{start} {start}\"] # индекс начальной биграммы\n",
        "\n",
        "    for _ in range(n):\n",
        "\n",
        "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
        "\n",
        "        next_word = id2word[chosen]\n",
        "        text.append(next_word)\n",
        "\n",
        "        if next_word == '<end>':\n",
        "            text.extend([start, start])\n",
        "            current_idx = bigram2id[f\"{start} {start}\"]\n",
        "        else:\n",
        "            bigram = f\"{text[-2]} {text[-1]}\"\n",
        "            current_idx = bigram2id.get(bigram, bigram2id[f\"{start} {start}\"])\n",
        "\n",
        "    text = ' '.join(text)\n",
        "    text = text.replace('<start> <start>', '')\n",
        "    text = text.replace('<end>', '\\n')\n",
        "    return text"
      ],
      "metadata": {
        "id": "eI13HExP9x-2"
      },
      "id": "eI13HExP9x-2",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, 6):\n",
        "    print('Генерация ' + str(i) + ':')\n",
        "    print(generate(matrix_corpus, id2word_corpus, word2id_corpus, id2bigram_corpus, bigram2id_corpus).replace('<end>', '\\n'))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2Wb9BND92Tl",
        "outputId": "50589d19-aa74-4966-cf95-48abbe95c488"
      },
      "id": "v2Wb9BND92Tl",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Генерация 1:\n",
            " до сих пор регистрация доменных имен в собственные руки и в машине \n",
            "  только одно из произведенных в великобритании самым теряемым предметом считался зонтик это даже было своего рода официальным аналитиком кремля и татьяны дьяченко и валерия окулова \n",
            "  благодарю бога что остался жив \n",
            "  является доцентом кафедры статистики имеет ученую степень доктора экономических наук \n",
            "  лицензии на осуществление банковских операций по счетам орт уже с самого начала \n",
            "  вывод телеведущего в ближайшие дни подать апелляцию по этому вопросу \n",
            "  кроме того масхадов заявил что в документах указанные погрешности и спустя 10 лет и старше чуть болеше обеспокоены перспективами в\n",
            "\n",
            "Генерация 2:\n",
            " уровень процентных ставок со сроком 5 месяцев до 1 июля до 18 часов \n",
            "  в результате была пресечена деятельность 127 опг задержаны пять воров в законе \n",
            "  убийца был одет в черную куртку черные джинсы и вязанную черную шапку надвинутую на глаза похитителям когда искали работу \n",
            "  как объяснили тогда районные власти стена была призвана защитить интересы чешских жителей которые согласятся их принять сославшись на отсутствие в гувд “ концепции развития россии в борьбе с коррупцией \n",
            "  за последнее время в республике традиционно отмечается второго ноября \n",
            "  как мы сообщали ранее 22 процентов \n",
            "  как сообщает агентство интерфакс-афи со ссылкой\n",
            "\n",
            "Генерация 3:\n",
            " в восьмидесятых годах санчес организовал нападение на директора советского цбк николая скубу примерно в шесть часов утра по московскому времени ураган находился примерно в 14 миллионов 677 тысяч 204 рубля у него не вступятся высокие покровители \n",
            "  в первую очередь углекислого газа \n",
            "  загрязнение окружающей среды из 173 стран мира включая ирак уже вывели микробы для уничтожения комаров которые разносят болезнь \n",
            "  швейцарская народная партия возглавляемая националистом кристофом блохером впервые набрала 23 процента голосов отданных за этот период был отмечен всего две недели на возвращение задолженностей \n",
            "  в последнее время заметно пошли вверх \n",
            "  на третьей строчке этого шутливого хит-парада\n",
            "\n",
            "Генерация 4:\n",
            " он может хранить в сотни раз больше смол чем обычные сигареты \n",
            "  самолет ан-14 принадлежащий федерации любителей авиации мап не вышел на связь военный вертолет ми-8 \n",
            "  в то время как в иностранной валюте \n",
            "  примечательно что протест талибана обращен к оон за содействием \n",
            "  поезда сошли с рельсов один из которых россия и союза правых сил сергеем кириенко \n",
            "  причины аварии до сих пор \n",
            "  впрочем удовольствие выпить с утра началась проверка паспортного режима \n",
            "  с учетом того что анатолий собчак выиграл процесс к по иску разделят гонорар в 122 миллиона долларов \n",
            "  правоохранительные органы о замеченных под столами\n",
            "\n",
            "Генерация 5:\n",
            " в то же время модель ваз-21102 подорожала примерно на сутки из тирасполя вышел эшелон с нефтью пошлина на газ виктор христенко провел встречу с представителем басаева \n",
            "  во время судебного разбирательства по антимонопольному иску предъявленному министерством юстиции сша дженет рино министр обороны грузии агентству военных новостей источник в прокуратуре московского военного округа \n",
            "  подземный толчок силой 5,2 балла по шкале рихтера \n",
            "  из-за ограничений накладываемых российским законодательством для местных сборов эта мера необходима для исключения инфицированности вич вирусами гепатита и возбудителем сифилиса \n",
            "  как выяснилось что этим электронным адресом пользовался человек имеющий доступ в интернет с использованием огромного количества оружия\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перплексия"
      ],
      "metadata": {
        "id": "WPJ3zjmyXc9w"
      },
      "id": "WPJ3zjmyXc9w"
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для расчёта перплексии\n",
        "def perplexity(logp, N):\n",
        "    return np.exp((-1 / N) * logp)"
      ],
      "metadata": {
        "id": "ZT6BlUYo3pWa"
      },
      "id": "ZT6BlUYo3pWa",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_join_proba_markov_assumption(text, bigram_counts, trigram_counts):\n",
        "    prob = 0\n",
        "    tokens = normalize(text)\n",
        "    tokens = ['<start>', '<start>'] + tokens + ['<end>']  # Добавляем маркеры начала и конца\n",
        "    for trigram in ngrammer(tokens, n=3):\n",
        "        word1, word2, word3 = trigram.split()\n",
        "        bigram = f\"{word1} {word2}\"\n",
        "        if bigram in bigram_counts and trigram in trigram_counts:\n",
        "            # Условная вероятность P(w3 | w1, w2)\n",
        "            prob += np.log(trigram_counts[trigram] / bigram_counts[bigram])\n",
        "        else:\n",
        "            # Сглаживание для отсутствующих триграмм\n",
        "            prob += np.log(2e-5)\n",
        "    return prob, len(tokens) - 3  # Возвращаем лог-вероятность и количество слов (без <start> <start>)"
      ],
      "metadata": {
        "id": "19QO-is17fQP"
      },
      "id": "19QO-is17fQP",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = []\n",
        "for sent in held_out_sentences:\n",
        "    # Склеиваем токены в строку, если sent — это список\n",
        "    if isinstance(sent, list):\n",
        "        sent = ' '.join(sent)\n",
        "\n",
        "    prob, N = compute_join_proba_markov_assumption(sent, bigrams_corpus, trigrams_corpus)\n",
        "    if not N:  # Пропускаем пустые предложения\n",
        "        continue\n",
        "    ps.append(perplexity(prob, N))"
      ],
      "metadata": {
        "id": "jbXDTCPH7fCn"
      },
      "id": "jbXDTCPH7fCn",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Средняя перплексия\n",
        "np.mean(ps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW87a6WX7mae",
        "outputId": "d7decadf-3075-4147-c2a4-25545a47179f"
      },
      "id": "eW87a6WX7mae",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50460.61994295969"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0a8dd5",
      "metadata": {
        "id": "8e0a8dd5"
      },
      "source": [
        "## Задание № 2* (2 балла)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f733858c",
      "metadata": {
        "id": "f733858c"
      },
      "source": [
        "Измените функцию generate_with_beam_search так, чтобы она работала с моделью, которая учитывает два предыдущих слова.\n",
        "Сравните получаемый результат с первым заданием.\n",
        "Также попробуйте начинать генерацию не с нуля (подавая \\<start> \\<start>), а с какого-то промпта. Но помните, что учитываться будут только два последних слова, так что не делайте длинные промпты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "c426746a",
      "metadata": {
        "id": "c426746a"
      },
      "outputs": [],
      "source": [
        "# сделаем класс чтобы хранить каждый из лучей\n",
        "class Beam:\n",
        "    def __init__(self, sequence: list, score: float):\n",
        "        self.sequence: list = sequence\n",
        "        self.score: float = score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_beam_search(matrix, id2word, word2id, id2bigram, bigram2id, n=100, max_beams=5, start='<start> <start>', prompt=None):\n",
        "    # Начальное состояние\n",
        "    if prompt:\n",
        "        # Если задан промпт, преобразуем его в список слов\n",
        "        initial_sequence = normalize(prompt)\n",
        "        if len(initial_sequence) < 2:\n",
        "            initial_sequence = ['<start>'] + initial_sequence\n",
        "        else:\n",
        "            initial_sequence = initial_sequence[-2:]  # Берём только два последних слова\n",
        "    else:\n",
        "        initial_sequence = ['<start>', '<start>']\n",
        "\n",
        "    initial_node = Beam(sequence=initial_sequence, score=0.0)\n",
        "    beams = [initial_node]\n",
        "\n",
        "    for _ in range(n):\n",
        "        # Список новых лучей\n",
        "        new_beams = []\n",
        "        for beam in beams:\n",
        "            # Если последовательность уже закончена, оставляем её без изменений\n",
        "            if beam.sequence[-1] == '<end>':\n",
        "                new_beams.append(beam)\n",
        "                continue\n",
        "\n",
        "            # Получаем последние два слова из последовательности\n",
        "            if len(beam.sequence) < 2:\n",
        "                bigram = '<start> <start>'\n",
        "            else:\n",
        "                bigram = f\"{beam.sequence[-2]} {beam.sequence[-1]}\"\n",
        "\n",
        "            # Проверяем наличие биграммы\n",
        "            if bigram not in bigram2id:\n",
        "                continue\n",
        "\n",
        "            # Получаем индекс биграммы\n",
        "            bigram_idx = bigram2id[bigram]\n",
        "\n",
        "            # Вероятности продолжений\n",
        "            probas = matrix[bigram_idx].toarray()[0]\n",
        "\n",
        "            # Выбираем топ самых вероятных продолжений\n",
        "            top_idxs = probas.argsort()[:-(max_beams+1):-1]\n",
        "\n",
        "            for top_id in top_idxs:\n",
        "                # Пропускаем нулевые вероятности\n",
        "                if not probas[top_id]:\n",
        "                    continue\n",
        "\n",
        "                # Создаём новый луч\n",
        "                new_sequence = beam.sequence + [id2word[top_id]]\n",
        "                new_score = beam.score + np.log(probas[top_id])\n",
        "                new_beam = Beam(sequence=new_sequence, score=new_score)\n",
        "                new_beams.append(new_beam)\n",
        "\n",
        "        # Сортируем и берём только `max_beams` лучших лучей\n",
        "        beams = sorted(new_beams, key=lambda x: x.score, reverse=True)[:max_beams]\n",
        "\n",
        "    # Возвращаем отсортированные последовательности по вероятности\n",
        "    sorted_sequences = sorted(beams, key=lambda x: x.score, reverse=True)\n",
        "    sorted_sequences = [\" \".join(beam.sequence).replace('<start>', '').strip() for beam in sorted_sequences]\n",
        "    return sorted_sequences"
      ],
      "metadata": {
        "id": "EIwqneJw_Lsd"
      },
      "id": "EIwqneJw_Lsd",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерация с началом по умолчанию\n",
        "generated_sequences = generate_with_beam_search(\n",
        "    matrix_corpus, id2word_corpus, word2id_corpus, id2bigram_corpus, bigram2id_corpus,\n",
        "    n=50, max_beams=5\n",
        ")\n",
        "print(\"Сгенерированные последовательности (с <start>):\")\n",
        "for seq in generated_sequences:\n",
        "    print(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBe9rCxs_PR2",
        "outputId": "627d6c9b-47d9-4b5a-8e65-68c20ed47c6b"
      },
      "id": "vBe9rCxs_PR2",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сгенерированные последовательности (с <start>):\n",
            "как сообщает риа новости <end>\n",
            "как сообщает агентство риа новости <end>\n",
            "как сообщает риа новости со ссылкой на источники в правоохранительных органах грузии <end>\n",
            "как сообщает риа новости со ссылкой на источники в правоохранительных органах столицы <end>\n",
            "как сообщает риа новости со ссылкой на источники в правоохранительных органах города интерфаксу сообщили в пресс-службе президента армении <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерация с промптом\n",
        "prompt = \"на днях\"\n",
        "generated_sequences_with_prompt = generate_with_beam_search(\n",
        "    matrix_corpus, id2word_corpus, word2id_corpus, id2bigram_corpus, bigram2id_corpus,\n",
        "    n=50, max_beams=5, prompt=prompt\n",
        ")\n",
        "print(\"Сгенерированные последовательности (с промптом):\")\n",
        "for seq in generated_sequences_with_prompt:\n",
        "    print(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScM3gHWh_cDv",
        "outputId": "3a6095e5-ed66-41c5-8ce3-a9d2fd94f508"
      },
      "id": "ScM3gHWh_cDv",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сгенерированные последовательности (с промптом):\n",
            "на днях в железногорске состоялось торжественное открытие международного делового центра <end>\n",
            "на днях в железногорске состоялось торжественное открытие международного делового центра москва-сити <end>\n",
            "на днях в селении дучи новолакского района дагестана <end>\n",
            "на днях в селении верхний наур было убито 170 человек <end>\n",
            "на днях в селении верхний наур было убито и шестеро солдат <end>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}