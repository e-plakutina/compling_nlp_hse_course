{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dba7c0d",
   "metadata": {
    "id": "1dba7c0d"
   },
   "source": [
    "# Домашнее задание № 10. Машинный перевод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yj7aripVIsbG",
   "metadata": {
    "id": "Yj7aripVIsbG"
   },
   "source": [
    "## Задание 1 (6 баллов + 2 доп балла).\n",
    "Нужно обучить трансформер на том же корпусе но в другую сторону - с русского на английский.\n",
    "Можно использовать как основу первый или второй способ реализации (с MultiheadAttention или с nn.Transformer). Подберите несколько тестовых примеров для проверки обучения на каждой эпохе.\n",
    "\n",
    "Параметры ниже точно работают в колабе и модель обучается достаточно быстро. Попробуйте их немного увеличить (batch size возможно придется наоборот уменьшить). Обучайте модель хотя бы 5 эпох, а желательно больше, чтобы тестовые примеры начали переводиться более менее адекватно.\n",
    "\n",
    "После обучения возьмите хотя бы 100 примером из тестовой части параллельного корпуса и переведите их. Оцените качество переводов с помощью метрики BLEU (пример использования ниже)\n",
    "Найдите лучшие (как минимум 5) переводы согласно этой метрике и проверьте действительно ли они хорошие. Если все переводы нулевые, то пообучайте модель подольше.\n",
    "\n",
    "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Сейчас она работает только с одним текстом - это не эффективно. Можно генерировать переводы сразу для нескольких текстов (батча). Главная сложность с таким подходом состоит в том, что генерируемые тексты будут заканчиваться в разное время и нужно сделать столько итераций, сколько нужно для завершения всех текстов (т.е. условие на то, что последний токен не равен [EOS] в текущем коде не сработает).\n",
    "ВАЖНО - недостаточно просто изменить входной аргумент с text на texts и добавить еще один цикл по texts! Сама модель должна вызываться на нескольких текстах! Функция с batch prediction должна работать быстрее, поэтому переведите всю тестовую выборку и оцените качество BLEU на всех данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d202c4",
   "metadata": {
    "id": "05d202c4"
   },
   "outputs": [],
   "source": [
    "# В колабе установите torchtune и torchao, чтобы семинарская тетрадка работала\n",
    "!pip install torchtune torchao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f35e4e",
   "metadata": {
    "id": "e8f35e4e"
   },
   "outputs": [],
   "source": [
    "# пример использования BLEU\n",
    "# обратите внимание что текты должны быть токенизированы\n",
    "import nltk\n",
    "\n",
    "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room'] # замените на перевод вашей модели\n",
    "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room'] # замените на эталонный перевод\n",
    "\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, auto_reweigh=True)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5efa8f8",
   "metadata": {
    "id": "b5efa8f8"
   },
   "outputs": [],
   "source": [
    "# параметры которые работают в колабе\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "ff_dim = embed_dim*2\n",
    "num_layers = 2\n",
    "batch_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "RlNOcpCsMiJ4"
   },
   "id": "RlNOcpCsMiJ4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "from torch.nn import Transformer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "id": "awkTDBLx-E6x"
   },
   "id": "awkTDBLx-E6x",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Данные:"
   ],
   "metadata": {
    "id": "NnKqCIh5qE6y"
   },
   "id": "NnKqCIh5qE6y"
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_uUQcPo_Ktm",
    "outputId": "b3c02887-b2b7-4cbe-b425-7c74b860c428"
   },
   "id": "j_uUQcPo_Ktm",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2025-04-01 15:46:39--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 121340806 (116M)\n",
      "Saving to: ‘opus.en-ru-train.ru’\n",
      "\n",
      "opus.en-ru-train.ru 100%[===================>] 115.72M  20.8MB/s    in 7.0s    \n",
      "\n",
      "2025-04-01 15:46:47 (16.6 MB/s) - ‘opus.en-ru-train.ru’ saved [121340806/121340806]\n",
      "\n",
      "--2025-04-01 15:46:47--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 67760131 (65M)\n",
      "Saving to: ‘opus.en-ru-train.en’\n",
      "\n",
      "opus.en-ru-train.en 100%[===================>]  64.62M  17.0MB/s    in 4.5s    \n",
      "\n",
      "2025-04-01 15:46:52 (14.3 MB/s) - ‘opus.en-ru-train.en’ saved [67760131/67760131]\n",
      "\n",
      "--2025-04-01 15:46:52--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 305669 (299K)\n",
      "Saving to: ‘opus.en-ru-test.ru’\n",
      "\n",
      "opus.en-ru-test.ru  100%[===================>] 298.50K   324KB/s    in 0.9s    \n",
      "\n",
      "2025-04-01 15:46:54 (324 KB/s) - ‘opus.en-ru-test.ru’ saved [305669/305669]\n",
      "\n",
      "--2025-04-01 15:46:54--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 173307 (169K)\n",
      "Saving to: ‘opus.en-ru-test.en’\n",
      "\n",
      "opus.en-ru-test.en  100%[===================>] 169.25K   234KB/s    in 0.7s    \n",
      "\n",
      "2025-04-01 15:46:56 (234 KB/s) - ‘opus.en-ru-test.en’ saved [173307/173307]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
    "f = open('opus.en-ru-train.ru', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ],
   "metadata": {
    "id": "eNV3kK89_PYk"
   },
   "id": "eNV3kK89_PYk",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ru_sents = open('opus.en-ru-train.ru').read().splitlines()\n",
    "en_sents = open('opus.en-ru-train.en').read().splitlines()"
   ],
   "metadata": {
    "id": "kJXKYZ1a_bm3"
   },
   "id": "kJXKYZ1a_bm3",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "ff_dim = embed_dim*2\n",
    "num_layers = 2\n",
    "batch_size = 400\n",
    "max_len_ru, max_len_en = 47, 48\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "id": "zvIaqhbN_eTS"
   },
   "id": "zvIaqhbN_eTS",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer_ru = Tokenizer(BPE())\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "trainer_ru = BpeTrainer(special_tokens=[\"[PAD]\"], end_of_word_suffix='</w>')\n",
    "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru)\n",
    "\n",
    "tokenizer_en = Tokenizer(BPE())\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "trainer_en = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], end_of_word_suffix='</w>')\n",
    "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en)"
   ],
   "metadata": {
    "id": "WnfrfNW9_klI"
   },
   "id": "WnfrfNW9_klI",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer_ru.decoder = decoders.BPEDecoder()\n",
    "tokenizer_en.decoder = decoders.BPEDecoder()"
   ],
   "metadata": {
    "id": "zVRIqccB_peY"
   },
   "id": "zVRIqccB_peY",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# tokenizer_ru.save('tokenizer_ru')\n",
    "# tokenizer_en.save('tokenizer_en')"
   ],
   "metadata": {
    "id": "z_4fkyZg_rdg"
   },
   "id": "z_4fkyZg_rdg",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")\n",
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")"
   ],
   "metadata": {
    "id": "GIj-QXPI_tq6"
   },
   "id": "GIj-QXPI_tq6",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "PAD_IDX = tokenizer_en.token_to_id('[PAD]')\n",
    "# BOS_IDX = tokenizer_en.token_to_id('[BOS]')\n",
    "# EOS_IDX = tokenizer_en.token_to_id('[EOS]')\n",
    "PAD_IDX"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LY9Bo7Yk_vfD",
    "outputId": "1b619ca6-3ad0-49be-fb45-939f2a6f4461"
   },
   "id": "LY9Bo7Yk_vfD",
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def encode(text, tokenizer, max_len, encoder=False):\n",
    "    if encoder:\n",
    "        return tokenizer.encode(text).ids[:max_len]\n",
    "    else:\n",
    "        return [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[EOS]')]"
   ],
   "metadata": {
    "id": "qwFcnIPaC71A"
   },
   "id": "qwFcnIPaC71A",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_ru = [encode(t, tokenizer_ru, max_len_ru, encoder=True) for t in ru_sents]\n",
    "X_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]"
   ],
   "metadata": {
    "id": "cKHtqu5P_8QP"
   },
   "id": "cKHtqu5P_8QP",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_ru_train, X_ru_valid, X_en_train, X_en_valid = train_test_split(X_ru, X_en, test_size=0.05)"
   ],
   "metadata": {
    "id": "xDo2k9B4_-lw"
   },
   "id": "xDo2k9B4_-lw",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts_ru, texts_en):\n",
    "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
    "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
    "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "        self.length = len(texts_ru)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ids_ru = self.texts_ru[index]\n",
    "        ids_en = self.texts_en[index]\n",
    "        return ids_ru, ids_en"
   ],
   "metadata": {
    "id": "ofrtjefoAAvQ"
   },
   "id": "ofrtjefoAAvQ",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "training_set = Dataset(X_ru_train, X_en_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_set = Dataset(X_ru_valid, X_en_valid)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "id": "L7Qu5q6UAEyY"
   },
   "id": "L7Qu5q6UAEyY",
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Трансформер:"
   ],
   "metadata": {
    "id": "wtrOSYLNqLce"
   },
   "id": "wtrOSYLNqLce"
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim)\n",
    "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim)\n",
    "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads, max_seq_len=128)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src_embedded = self.embedding_enc(src)\n",
    "        B,S,E = src_embedded.shape\n",
    "        src_embedded = self.positional_encoding(src_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
    "\n",
    "        tgt_embedded = self.embedding_dec(tgt)\n",
    "        B,S,E = tgt_embedded.shape\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
    "\n",
    "        tgt_mask = (~torch.tril(torch.ones((S, S), dtype=torch.bool))).to(DEVICE)\n",
    "\n",
    "        encoder_output = self.transformer.encoder(\n",
    "            src_embedded,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "        decoder_output = self.transformer.decoder(\n",
    "            tgt_embedded,\n",
    "            encoder_output,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "        output = self.output_layer(decoder_output)\n",
    "        return output"
   ],
   "metadata": {
    "id": "Iu58jD8sAIxV"
   },
   "id": "Iu58jD8sAIxV",
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab_size_enc = tokenizer_ru.get_vocab_size()\n",
    "vocab_size_dec = tokenizer_en.get_vocab_size()"
   ],
   "metadata": {
    "id": "Kwb7n4xcAQZX"
   },
   "id": "Kwb7n4xcAQZX",
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = TransformerEncoderDecoder(vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)"
   ],
   "metadata": {
    "id": "Kd9X6IY4ASwr"
   },
   "id": "Kd9X6IY4ASwr",
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrDga1GVAWmw",
    "outputId": "2347acf7-3ff7-413e-cb60-7e81eb16a91d"
   },
   "id": "CrDga1GVAWmw",
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.95288 M parameters\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, iterator, optimizer, criterion, scheduler, print_every=100):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for i, (texts_ru, texts_en) in enumerate(iterator):\n",
    "        texts_ru = texts_ru.to(DEVICE)\n",
    "        texts_en = texts_en.to(DEVICE)\n",
    "        texts_en_input = texts_en[:,:-1].to(DEVICE)\n",
    "        texts_en_out = texts_en[:, 1:].to(DEVICE)\n",
    "        src_padding_mask = (texts_ru == PAD_IDX).to(DEVICE)\n",
    "        tgt_padding_mask = (texts_en_input == PAD_IDX).to(DEVICE)\n",
    "\n",
    "        logits = model(texts_ru, texts_en_input, src_padding_mask, tgt_padding_mask)\n",
    "        optimizer.zero_grad()\n",
    "        B,S,C = logits.shape\n",
    "        loss = loss_fn(logits.reshape(B*S, C), texts_en_out.reshape(B*S))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        if not (i+1) % print_every:\n",
    "            print(f'Loss: {np.mean(epoch_loss)};')\n",
    "\n",
    "    return np.mean(epoch_loss)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (texts_ru, texts_en) in enumerate(iterator):\n",
    "            texts_ru = texts_ru.to(DEVICE)\n",
    "            texts_en = texts_en.to(DEVICE)\n",
    "            texts_en_input = texts_en[:,:-1].to(DEVICE)\n",
    "            texts_en_out = texts_en[:, 1:].to(DEVICE)\n",
    "            src_padding_mask = (texts_ru == PAD_IDX).to(DEVICE)\n",
    "            tgt_padding_mask = (texts_en_input == PAD_IDX).to(DEVICE)\n",
    "\n",
    "            logits = model(texts_ru, texts_en_input, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "            B,S,C = logits.shape\n",
    "            loss = loss_fn(logits.reshape(B*S, C), texts_en_out.reshape(B*S))\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ],
   "metadata": {
    "id": "AeDEXS8qAXIX"
   },
   "id": "AeDEXS8qAXIX",
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Функция перевода:"
   ],
   "metadata": {
    "id": "cTlse4HkqP8g"
   },
   "id": "cTlse4HkqP8g"
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad\n",
    "def translate(text):\n",
    "    input_ids = tokenizer_ru.encode(text).ids[:max_len_ru]\n",
    "    output_ids = [tokenizer_en.token_to_id('[BOS]')]\n",
    "\n",
    "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)], batch_first=True).to(DEVICE)\n",
    "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
    "\n",
    "    src_padding_mask = (input_ids_pad == PAD_IDX).to(DEVICE)\n",
    "    tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
    "\n",
    "    logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "    pred = logits.argmax(2).item()\n",
    "\n",
    "    while pred not in [tokenizer_en.token_to_id('[EOS]'), tokenizer_en.token_to_id('[PAD]')] and len(output_ids) < 100:\n",
    "        output_ids.append(pred)\n",
    "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
    "        tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
    "        pred = logits.argmax(2).view(-1)[-1].item()\n",
    "\n",
    "    return tokenizer_en.decoder.decode([tokenizer_en.id_to_token(i) for i in output_ids[1:]])"
   ],
   "metadata": {
    "id": "5dcyqCxqAbo5"
   },
   "id": "5dcyqCxqAbo5",
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучение:"
   ],
   "metadata": {
    "id": "N-uEJNx3qUvL"
   },
   "id": "N-uEJNx3qUvL"
  },
  {
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, pct_start=0.05,\n",
    "                                              steps_per_epoch=len(training_generator), epochs=NUM_EPOCHS)\n",
    "\n",
    "losses = []\n",
    "test_examples = [\n",
    "    \"Да, но не совсем...\",\n",
    "    \"Привет, как дела?\",\n",
    "    \"Это пример перевода.\",\n",
    "    \"Я изучаю машинное обучение.\",\n",
    "    \"Трансформеры - это мощные модели.\"\n",
    "]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train(model, training_generator, optimizer, loss_fn, scheduler)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model, valid_generator, loss_fn)\n",
    "\n",
    "    if not losses or val_loss < min(losses):\n",
    "        print(f'Improved to {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "\n",
    "    losses.append(val_loss)\n",
    "\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "          f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
    "\n",
    "    # Print test examples\n",
    "    print(\"\\nTest translations:\")\n",
    "    for example in test_examples:\n",
    "        print(f\"RU: {example}\")\n",
    "        print(f\"EN: {translate(example)}\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Ql4NT09AgnK",
    "outputId": "44fc9c54-c983-4732-e575-90c8fee372ff"
   },
   "id": "-Ql4NT09AgnK",
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 10.355045833587646;\n",
      "Loss: 10.167172012329102;\n",
      "Loss: 9.935965979894002;\n",
      "Loss: 9.628733744621277;\n",
      "Loss: 9.24804971408844;\n",
      "Loss: 8.881004736423492;\n",
      "Loss: 8.586942828042167;\n",
      "Loss: 8.33937436044216;\n",
      "Loss: 8.124135661125184;\n",
      "Loss: 7.935873209953308;\n",
      "Loss: 7.76853129863739;\n",
      "Loss: 7.620374661684036;\n",
      "Loss: 7.488636261866643;\n",
      "Loss: 7.3691024879046845;\n",
      "Loss: 7.260065105438232;\n",
      "Loss: 7.16180088698864;\n",
      "Loss: 7.071753857276019;\n",
      "Loss: 6.988029187255435;\n",
      "Loss: 6.911399175242374;\n",
      "Loss: 6.840401908874512;\n",
      "Loss: 6.7749978633154;\n",
      "Loss: 6.713493946032091;\n",
      "Loss: 6.656047990425773;\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Improved to 5.283758491516113, saving model..\n",
      "Epoch: 1, Train loss: 6.615, Val loss: 5.284, Epoch time=418.501s\n",
      "\n",
      "Test translations:\n",
      "RU: Да, но не совсем...\n",
      "EN: No , no , but I ' m not not not not not not not not not .\n",
      "\n",
      "RU: Привет, как дела?\n",
      "EN: You ' re going ?\n",
      "\n",
      "RU: Это пример перевода.\n",
      "EN: It ' s a good .\n",
      "\n",
      "RU: Я изучаю машинное обучение.\n",
      "EN: I ' m a little .\n",
      "\n",
      "RU: Трансформеры - это мощные модели.\n",
      "EN: We ' re gonna be a little .\n",
      "\n",
      "Loss: 5.3318988227844235;\n",
      "Loss: 5.314874973297119;\n",
      "Loss: 5.297300187746684;\n",
      "Loss: 5.284473857879639;\n",
      "Loss: 5.272793460845947;\n",
      "Loss: 5.264507575829824;\n",
      "Loss: 5.2547317280088155;\n",
      "Loss: 5.2438782924413685;\n",
      "Loss: 5.232617939313252;\n",
      "Loss: 5.221666922092438;\n",
      "Loss: 5.211262934424661;\n",
      "Loss: 5.2012636586030325;\n",
      "Loss: 5.191612660334661;\n",
      "Loss: 5.181989494051252;\n",
      "Loss: 5.17279104423523;\n",
      "Loss: 5.164083296954632;\n",
      "Loss: 5.155127008662505;\n",
      "Loss: 5.146764573785994;\n",
      "Loss: 5.139076070534555;\n",
      "Loss: 5.131712389230728;\n",
      "Loss: 5.123841428756714;\n",
      "Loss: 5.11631867538799;\n",
      "Loss: 5.1090807981076445;\n",
      "Improved to 4.850251449584961, saving model..\n",
      "Epoch: 2, Train loss: 5.104, Val loss: 4.850, Epoch time=422.747s\n",
      "\n",
      "Test translations:\n",
      "RU: Да, но не совсем...\n",
      "EN: Yeah , no , but ...\n",
      "\n",
      "RU: Привет, как дела?\n",
      "EN: Hey , okay ?\n",
      "\n",
      "RU: Это пример перевода.\n",
      "EN: It ' s a good .\n",
      "\n",
      "RU: Я изучаю машинное обучение.\n",
      "EN: I ' m a little bit .\n",
      "\n",
      "RU: Трансформеры - это мощные модели.\n",
      "EN: The time of the same time .\n",
      "\n",
      "Loss: 4.9055036926269535;\n",
      "Loss: 4.904671397209167;\n",
      "Loss: 4.8978850412368775;\n",
      "Loss: 4.8983849740028385;\n",
      "Loss: 4.895925529479981;\n",
      "Loss: 4.8911135927836105;\n",
      "Loss: 4.885861477851868;\n",
      "Loss: 4.881015264987946;\n",
      "Loss: 4.877245649231805;\n",
      "Loss: 4.8721478905677795;\n",
      "Loss: 4.8682695904645055;\n",
      "Loss: 4.863828993638356;\n",
      "Loss: 4.859753914246192;\n",
      "Loss: 4.856007238796779;\n",
      "Loss: 4.851061753908793;\n",
      "Loss: 4.8471118775010105;\n",
      "Loss: 4.843443181654986;\n",
      "Loss: 4.839840001000298;\n",
      "Loss: 4.836720316786515;\n",
      "Loss: 4.832460572004318;\n",
      "Loss: 4.828663272857666;\n",
      "Loss: 4.82544080582532;\n",
      "Loss: 4.822297621602598;\n",
      "Improved to 4.661692150115967, saving model..\n",
      "Epoch: 3, Train loss: 4.820, Val loss: 4.662, Epoch time=421.779s\n",
      "\n",
      "Test translations:\n",
      "RU: Да, но не совсем...\n",
      "EN: Yeah , but it ' s not ...\n",
      "\n",
      "RU: Привет, как дела?\n",
      "EN: Hey , okay ?\n",
      "\n",
      "RU: Это пример перевода.\n",
      "EN: It ' s a good .\n",
      "\n",
      "RU: Я изучаю машинное обучение.\n",
      "EN: I ' m a little bit .\n",
      "\n",
      "RU: Трансформеры - это мощные модели.\n",
      "EN: The only one of the same time .\n",
      "\n",
      "Loss: 4.726329231262207;\n",
      "Loss: 4.725698537826538;\n",
      "Loss: 4.717151350975037;\n",
      "Loss: 4.716306246519089;\n",
      "Loss: 4.7143209819793706;\n",
      "Loss: 4.710112308661143;\n",
      "Loss: 4.7079532418932235;\n",
      "Loss: 4.705989897847176;\n",
      "Loss: 4.7032630856831865;\n",
      "Loss: 4.70046262216568;\n",
      "Loss: 4.698367671099576;\n",
      "Loss: 4.697170278231303;\n",
      "Loss: 4.694869057215177;\n",
      "Loss: 4.692305220876421;\n",
      "Loss: 4.690466978391012;\n",
      "Loss: 4.687789773643017;\n",
      "Loss: 4.686252492736368;\n",
      "Loss: 4.683937828805711;\n",
      "Loss: 4.681324973106384;\n",
      "Loss: 4.67995924448967;\n",
      "Loss: 4.678106074560256;\n",
      "Loss: 4.675548297925429;\n",
      "Loss: 4.673585262920546;\n",
      "Improved to 4.5475824966430665, saving model..\n",
      "Epoch: 4, Train loss: 4.672, Val loss: 4.548, Epoch time=421.873s\n",
      "\n",
      "Test translations:\n",
      "RU: Да, но не совсем...\n",
      "EN: Yeah , but it ' s not ...\n",
      "\n",
      "RU: Привет, как дела?\n",
      "EN: Hey , you ' re going to go ?\n",
      "\n",
      "RU: Это пример перевода.\n",
      "EN: It ' s a good .\n",
      "\n",
      "RU: Я изучаю машинное обучение.\n",
      "EN: I ' m a new message .\n",
      "\n",
      "RU: Трансформеры - это мощные модели.\n",
      "EN: The only one of the world .\n",
      "\n",
      "Loss: 4.6096195173263546;\n",
      "Loss: 4.608377783298493;\n",
      "Loss: 4.606843795776367;\n",
      "Loss: 4.59957515001297;\n",
      "Loss: 4.600494574546814;\n",
      "Loss: 4.598709464073181;\n",
      "Loss: 4.597586081368583;\n",
      "Loss: 4.595457739830017;\n",
      "Loss: 4.592780779202779;\n",
      "Loss: 4.5925832929611206;\n",
      "Loss: 4.592247204780579;\n",
      "Loss: 4.59224875330925;\n",
      "Loss: 4.591009511580834;\n",
      "Loss: 4.589895154067448;\n",
      "Loss: 4.58815161037445;\n",
      "Loss: 4.58679070174694;\n",
      "Loss: 4.585567400315228;\n",
      "Loss: 4.583941706021627;\n",
      "Loss: 4.5822080009862;\n",
      "Loss: 4.580397602081299;\n",
      "Loss: 4.579281267211551;\n",
      "Loss: 4.578137218735435;\n",
      "Loss: 4.576018892578457;\n",
      "Improved to 4.473409397125244, saving model..\n",
      "Epoch: 5, Train loss: 4.575, Val loss: 4.473, Epoch time=422.851s\n",
      "\n",
      "Test translations:\n",
      "RU: Да, но не совсем...\n",
      "EN: Yeah , but it ' s not ...\n",
      "\n",
      "RU: Привет, как дела?\n",
      "EN: Hey , you ' re going to go ?\n",
      "\n",
      "RU: Это пример перевода.\n",
      "EN: It ' s a good .\n",
      "\n",
      "RU: Я изучаю машинное обучение.\n",
      "EN: I ' m a new .\n",
      "\n",
      "RU: Трансформеры - это мощные модели.\n",
      "EN: The problem is a new problem .\n",
      "\n",
      "Loss: 4.519386224746704;\n",
      "Loss: 4.523531241416931;\n",
      "Loss: 4.521412598292033;\n",
      "Loss: 4.520214395523071;\n",
      "Loss: 4.522768978118896;\n",
      "Loss: 4.52258539835612;\n",
      "Loss: 4.519616310255868;\n",
      "Loss: 4.519866665005684;\n",
      "Loss: 4.519916992717319;\n",
      "Loss: 4.520317795753479;\n",
      "Loss: 4.519894779812206;\n",
      "Loss: 4.518334400653839;\n",
      "Loss: 4.517056036362281;\n",
      "Loss: 4.515743859154838;\n",
      "Loss: 4.514721418698628;\n",
      "Loss: 4.514407503306866;\n",
      "Loss: 4.514472731421976;\n",
      "Loss: 4.514008126788669;\n",
      "Loss: 4.513142478089583;\n",
      "Loss: 4.511774021863937;\n",
      "Loss: 4.511173322314308;\n",
      "Loss: 4.510952151255174;\n",
      "Loss: 4.510119388829107;\n",
      "Improved to 4.42293404006958, saving model..\n",
      "Epoch: 6, Train loss: 4.509, Val loss: 4.423, Epoch time=421.689s\n",
      "\n",
      "Test translations:\n",
      "RU: Да, но не совсем...\n",
      "EN: Yeah , but it ' s not ...\n",
      "\n",
      "RU: Привет, как дела?\n",
      "EN: Hey , what ' s the hell ?\n",
      "\n",
      "RU: Это пример перевода.\n",
      "EN: It ' s a good .\n",
      "\n",
      "RU: Я изучаю машинное обучение.\n",
      "EN: I ' m a new .\n",
      "\n",
      "RU: Трансформеры - это мощные модели.\n",
      "EN: The problem is a new problem .\n",
      "\n",
      "Loss: 4.464004120826721;\n",
      "Loss: 4.463891098499298;\n",
      "Loss: 4.470934853553772;\n",
      "Loss: 4.470358136892319;\n",
      "Loss: 4.472872175216675;\n",
      "Loss: 4.471963581244151;\n",
      "Loss: 4.471472103255135;\n",
      "Loss: 4.471371006369591;\n",
      "Loss: 4.4730978960461085;\n",
      "Loss: 4.472083210468292;\n",
      "Loss: 4.472673584764654;\n",
      "Loss: 4.472099100748698;\n",
      "Loss: 4.4702226499410775;\n",
      "Loss: 4.4707947754859925;\n",
      "Loss: 4.46957853158315;\n",
      "Loss: 4.469685187935829;\n",
      "Loss: 4.46905126543606;\n",
      "Loss: 4.468058134714762;\n",
      "Loss: 4.467731569691709;\n",
      "Loss: 4.466975259304046;\n",
      "Loss: 4.466861400604248;\n",
      "Loss: 4.466063067913056;\n",
      "Loss: 4.465378612642703;\n",
      "Improved to 4.392626804351806, saving model..\n",
      "Epoch: 7, Train loss: 4.465, Val loss: 4.393, Epoch time=421.560s\n",
      "\n",
      "Test translations:\n",
      "RU: Да, но не совсем...\n",
      "EN: Yeah , but it ' s not ...\n",
      "\n",
      "RU: Привет, как дела?\n",
      "EN: Hey , what ' s right ?\n",
      "\n",
      "RU: Это пример перевода.\n",
      "EN: It ' s a good .\n",
      "\n",
      "RU: Я изучаю машинное обучение.\n",
      "EN: I ' m a new .\n",
      "\n",
      "RU: Трансформеры - это мощные модели.\n",
      "EN: The problem is a result of the world .\n",
      "\n",
      "Loss: 4.464181442260742;\n",
      "Loss: 4.4574037718772885;\n",
      "Loss: 4.4518362283706665;\n",
      "Loss: 4.4493311858177185;\n",
      "Loss: 4.4489546985626225;\n",
      "Loss: 4.445978568394978;\n",
      "Loss: 4.4447222457613265;\n",
      "Loss: 4.444540534615516;\n",
      "Loss: 4.443805552588569;\n",
      "Loss: 4.442793991565704;\n",
      "Loss: 4.442608140165156;\n",
      "Loss: 4.441742565234502;\n",
      "Loss: 4.441462374833914;\n",
      "Loss: 4.440664869717189;\n",
      "Loss: 4.439448707262675;\n",
      "Loss: 4.439811987280845;\n",
      "Loss: 4.438816393403446;\n",
      "Loss: 4.438451651467218;\n",
      "Loss: 4.438798272986161;\n",
      "Loss: 4.438690817356109;\n",
      "Loss: 4.4381361666179835;\n",
      "Loss: 4.437242291190407;\n",
      "Loss: 4.437645514115044;\n",
      "Improved to 4.374783756256104, saving model..\n",
      "Epoch: 8, Train loss: 4.438, Val loss: 4.375, Epoch time=422.703s\n",
      "\n",
      "Test translations:\n",
      "RU: Да, но не совсем...\n",
      "EN: Yeah , but it ' s not ...\n",
      "\n",
      "RU: Привет, как дела?\n",
      "EN: Hey , what ' s the hell ?\n",
      "\n",
      "RU: Это пример перевода.\n",
      "EN: It ' s a good .\n",
      "\n",
      "RU: Я изучаю машинное обучение.\n",
      "EN: I ' m a new .\n",
      "\n",
      "RU: Трансформеры - это мощные модели.\n",
      "EN: The need to be a new - and the most important .\n",
      "\n",
      "Loss: 4.423915629386902;\n",
      "Loss: 4.422617361545563;\n",
      "Loss: 4.424284671147665;\n",
      "Loss: 4.424590491056442;\n",
      "Loss: 4.424087165832519;\n",
      "Loss: 4.424532317320506;\n",
      "Loss: 4.424637545176915;\n",
      "Loss: 4.422752589583397;\n",
      "Loss: 4.4218715975019665;\n",
      "Loss: 4.421507394313812;\n",
      "Loss: 4.421051787029613;\n",
      "Loss: 4.420778458515803;\n",
      "Loss: 4.4209129447203415;\n",
      "Loss: 4.42099792616708;\n",
      "Loss: 4.422673800468445;\n",
      "Loss: 4.423256569802761;\n",
      "Loss: 4.42334094692679;\n",
      "Loss: 4.42314373254776;\n",
      "Loss: 4.423090932494715;\n",
      "Loss: 4.423414441347123;\n",
      "Loss: 4.422938885688782;\n",
      "Loss: 4.422862218510021;\n",
      "Loss: 4.422645354270935;\n",
      "Improved to 4.367698745727539, saving model..\n",
      "Epoch: 9, Train loss: 4.422, Val loss: 4.368, Epoch time=421.650s\n",
      "\n",
      "Test translations:\n",
      "RU: Да, но не совсем...\n",
      "EN: Yeah , but ...\n",
      "\n",
      "RU: Привет, как дела?\n",
      "EN: Hey , what ' s going ?\n",
      "\n",
      "RU: Это пример перевода.\n",
      "EN: It ' s a good .\n",
      "\n",
      "RU: Я изучаю машинное обучение.\n",
      "EN: I ' m a new .\n",
      "\n",
      "RU: Трансформеры - это мощные модели.\n",
      "EN: The problem is a result of the world .\n",
      "\n",
      "Loss: 4.413804497718811;\n",
      "Loss: 4.416881635189056;\n",
      "Loss: 4.418745719591777;\n",
      "Loss: 4.420198229551315;\n",
      "Loss: 4.4195640239715575;\n",
      "Loss: 4.4183785080909725;\n",
      "Loss: 4.419241455623082;\n",
      "Loss: 4.417733032107353;\n",
      "Loss: 4.417691826290555;\n",
      "Loss: 4.418281548500061;\n",
      "Loss: 4.416048871387135;\n",
      "Loss: 4.415934681495031;\n",
      "Loss: 4.415902020747845;\n",
      "Loss: 4.415593314852034;\n",
      "Loss: 4.416515167236328;\n",
      "Loss: 4.416586990952492;\n",
      "Loss: 4.417334826413323;\n",
      "Loss: 4.417052306599087;\n",
      "Loss: 4.416969470224882;\n",
      "Loss: 4.416601927518845;\n",
      "Loss: 4.416551581791469;\n",
      "Loss: 4.416384737058119;\n",
      "Loss: 4.416215790872989;\n",
      "Improved to 4.366847667694092, saving model..\n",
      "Epoch: 10, Train loss: 4.416, Val loss: 4.367, Epoch time=421.546s\n",
      "\n",
      "Test translations:\n",
      "RU: Да, но не совсем...\n",
      "EN: Yeah , but ...\n",
      "\n",
      "RU: Привет, как дела?\n",
      "EN: Hey , what ' s going ?\n",
      "\n",
      "RU: Это пример перевода.\n",
      "EN: It ' s a good .\n",
      "\n",
      "RU: Я изучаю машинное обучение.\n",
      "EN: I ' m a new .\n",
      "\n",
      "RU: Трансформеры - это мощные модели.\n",
      "EN: The problem is a result of the world .\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Метрика BLEU:"
   ],
   "metadata": {
    "id": "s3IH2tW-qYMj"
   },
   "id": "s3IH2tW-qYMj"
  },
  {
   "cell_type": "code",
   "source": [
    "ru_test = open('opus.en-ru-test.ru').read().replace('\\xa0', ' ').splitlines()[:100]\n",
    "en_test = open('opus.en-ru-test.en').read().splitlines()[:100]"
   ],
   "metadata": {
    "id": "wP5_3nSoAoMV"
   },
   "id": "wP5_3nSoAoMV",
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download(\"punkt_tab\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcHxdNqkUUVv",
    "outputId": "888c0419-a47b-4e95-e841-033f980dda9a"
   },
   "id": "rcHxdNqkUUVv",
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "bleu_scores = []\n",
    "translations = []\n",
    "\n",
    "for ru, en_ref in zip(ru_test, en_test):\n",
    "    translation = translate(ru)\n",
    "    translations.append((ru, en_ref, translation))\n",
    "\n",
    "    hyp_tokens = nltk.word_tokenize(translation.lower())\n",
    "    ref_tokens = nltk.word_tokenize(en_ref.lower())\n",
    "\n",
    "    bleu = nltk.translate.bleu_score.sentence_bleu([ref_tokens], hyp_tokens, auto_reweigh=True)\n",
    "    bleu_scores.append(bleu)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvgUpgKNAp7g",
    "outputId": "422a2eff-157d-4174-e2b9-624009b0a3a3"
   },
   "id": "cvgUpgKNAp7g",
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "top_indices = np.argsort(bleu_scores)[-5:][::-1]\n",
    "print(\"\\nTop 5 translations by BLEU score:\")\n",
    "for idx in top_indices:\n",
    "    ru, en_ref, en_hyp = translations[idx]\n",
    "    print(f\"\\nRussian: {ru}\")\n",
    "    print(f\"Reference EN: {en_ref}\")\n",
    "    print(f\"Translated EN: {en_hyp}\")\n",
    "    print(f\"BLEU: {bleu_scores[idx]:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage BLEU score: {np.mean(bleu_scores):.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yk-5gZAMAtnu",
    "outputId": "59795018-c010-4c62-f658-7b3fb3fb1c29"
   },
   "id": "yk-5gZAMAtnu",
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Top 5 translations by BLEU score:\n",
      "\n",
      "Russian: Да.\n",
      "Reference EN: - Yes.\n",
      "Translated EN: Yes .\n",
      "BLEU: 0.6065\n",
      "\n",
      "Russian: Я тебя люблю, папа.\n",
      "Reference EN: I love you, Dad. How great!\n",
      "Translated EN: I love you , Dad .\n",
      "BLEU: 0.6065\n",
      "\n",
      "Russian: Now, if you have kids yourself, you know what--\n",
      "Reference EN: Now, if you have kids yourself, you know what--\n",
      "Translated EN: You know , you know what you ' re gonna be a little bit of the world .\n",
      "BLEU: 0.1340\n",
      "\n",
      "Russian: Если вы хотите завершить этот процесс, вам понадобится минимум 8-10 лет, если не хотите рисковать тем, чтобы Пауни стал...\n",
      "Reference EN: If you want to see this through, you need eight to ten years minimum, unless you want to run the risk of Pawnee becoming...\n",
      "Translated EN: If you want to be a few years , I want to be a few years , I don ' t think that it ' t to be a lot of the time .\n",
      "BLEU: 0.0964\n",
      "\n",
      "Russian: b) что касается основного имущества, утраченного или поврежденного в результате единичного враждебного действия или вынужденного оставления, то Организация Объединенных Наций будет нести ответственность за каждую единицу основного имущества, РРС которого равна или превышает 250 000 долл. США, либо за утраченное или поврежденное основное имущество, когда совокупная РРС такого имущества равна или превышает 250 000 долл. США.\n",
      "Reference EN: (b) For major equipment lost or damaged as a result of single hostile action or forced abandonment, the United Nations will assume liability for each item of major equipment whose GFMV equals or exceeds $250,000 or for major equipment lost or damaged when the collective GFMV of such equipment equals or exceeds $250,000.\n",
      "Translated EN: ( b ) The State party was a result of the United States of the United States of the United States , the United States of the United Nations Convention , the United Nations of the United Nations , the United Nations , the United Nations ,\n",
      "BLEU: 0.0744\n",
      "\n",
      "Average BLEU score: 0.0152\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Переводы не очень, а один из примеров, который должен быть на русском, почему-то на английском...\n",
    "\n",
    "Но для очень простых предложений перевод довольно хороший (как для \"да\" и \"я тебя люблю, папа\"). Во втором примере странно, что в английском есть еще добавление \"How great!\", а в русском ничего подобного нет. Возможно в корпусе больше таких случаев, что в теории может влиять на качество модели.\n",
    "\n",
    "С увеличением количества эпох модель выдавала лучшие переводы для контрольных примеров. Если модель еще пообучать, качество тоже улучшилось бы"
   ],
   "id": "c9d5e0f0f32e3730"
  },
  {
   "cell_type": "markdown",
   "id": "b5aa93d6",
   "metadata": {
    "id": "b5aa93d6"
   },
   "source": [
    "\n",
    "## Задание 2 (2 балла).\n",
    "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/13.pdf\n",
    "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как ее применить к паре en->ru на данных из семинара. Сколько моделей понадобится? Сколько запусков обучения нужно будет сделать?\n",
    "\n",
    "Ответ должен содержать как минимум 10 предложений."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Back translation - это техника, которая используется в машинном переводе для аугментации данных. Она применяется, когда данных в параллельном корпусе мало, например, когда мы имеем дело с малоресурсными языками, или когда какой-то определенный жанр плохо представлен в датасете, но при этом у нас есть монолингвальные данные.\n",
    "\n",
    "Сначала мы используем параллельный корпус для обучения модели машинного перевода в обратном направлении (изначально мы хотим переводить из языка-источника в язык-таргет) - мы переводим из языка-таргета в язык-источник (a target-to-source MT system). Затем мы используем эту модель для перевода монолингвального корпуса (языка-таргета) в язык-источник.\n",
    "\n",
    "После этого мы объединяем естественно полученные данные с искусственными и обучаем модель, которая будет нам переводить язык-источник в язык-таргет.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Теперь разберем, как применить эту технику к данным из семинара.\n",
    "\n",
    "Языком-источником является английский, языком-таргетом - русский.\n",
    "\n",
    "У нас есть маленький пареллельный англо-русский корпус и много монолингвальных данных на русском.\n",
    "\n",
    "Нам понадобится 2 модели (одна переводит с английского на русский, вторая - с русского на английский).\n",
    "\n",
    "И нам надо проделать следующие шаги:\n",
    "\n",
    "\n",
    "1.   Обучаем модель, которая переводит в обратном направлении (с русского на английский) на параллельном корпусе.\n",
    "2.   При помощи этой же модели получаем больше параллельных данных, используя ее при переводе монолингвального корпуса (он на русском языке).\n",
    "1.   Объединяем естественно полученные данные и синтетические данные в один большой датасет.\n",
    "2.   Обучаем новую модель, которая переводит с языка-источника на язык-таргет, то есть с английского на русский. Мы улучшаем качество модели путем увеличения данных (мы обучаем модель на большом параллельном корпусе).\n",
    "\n",
    "Таким образом, необходимо будет сделать 3 запуска модели."
   ],
   "metadata": {
    "id": "s-0g2FPBoqoO"
   },
   "id": "s-0g2FPBoqoO"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
