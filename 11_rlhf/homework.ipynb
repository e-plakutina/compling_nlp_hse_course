{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9e6bbc",
   "metadata": {
    "id": "ec9e6bbc"
   },
   "source": [
    "# Задание 1 (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a0c17",
   "metadata": {
    "id": "ae4a0c17"
   },
   "source": [
    "Обучите языковую модель с помощью GRPO на вот этом датасете - `notbadai/python_functions_reasoning`\n",
    "Он чем-то похож на датасет из семинара, но вместо математических задач нужно написать код на питоне.\n",
    "Измените system prompt под эту задачу. Напишите аналогичные reward функции - одна на проверку формата (markdown wrappers ```python... ``` вокруг кода в конце), и на проверку что код работает (можете использовать import ast; ast.parse(string) для проверки)\n",
    "Если модель сначала не может отвечать в нужно формате, то сделайте цикл SFT обучения на небольшом куске датасета.\n",
    "GRPO в колабе может работать долго, поэтому не старайтесь пропустить весь датасет. Но обучайте хотя бы на 100 промптах (по несколько генераций на каждый промпт).\n",
    "Оцените разницу в качестве на небольшом сабсете, прогнав изначальную модель, модель после SFT (если она есть) и GRPO модель. В качестве метрики используйте reward функции.\n",
    "Можете взять любую языковую модель любого размера. Можете дообучать всю модель или же только адаптера поверх квантизированной модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U bitsandbytes"
   ],
   "metadata": {
    "id": "ygvPBORpAyUM"
   },
   "id": "ygvPBORpAyUM",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "id": "umpFrQida98q",
    "collapsed": true
   },
   "id": "umpFrQida98q",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install trl"
   ],
   "metadata": {
    "id": "v8qwlWqMbaof",
    "collapsed": true
   },
   "id": "v8qwlWqMbaof",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from trl import DPOTrainer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "import re\n",
    "import ast"
   ],
   "metadata": {
    "id": "eSxBmGksEyv2"
   },
   "id": "eSxBmGksEyv2",
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "device = 'cuda'"
   ],
   "metadata": {
    "id": "_dNInWXsaaYs"
   },
   "id": "_dNInWXsaaYs",
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ],
   "metadata": {
    "id": "thS-1rTLSOQZ"
   },
   "id": "thS-1rTLSOQZ",
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Необходимые функции:"
   ],
   "metadata": {
    "id": "HOI9OemwBsSz"
   },
   "id": "HOI9OemwBsSz"
  },
  {
   "cell_type": "code",
   "source": [
    "system_prompt = \"\"\"\n",
    "Your task is to write correct and efficient python code based on the user's task. Before outputting the result, give step-by-step reasoning. Always wrap the final code in markdown code blocks like this:\n",
    "```python\n",
    "# your final code here\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# system prompt задает формат ответа, в модель подается текст (то есть на выходе должен получиться только промпт)\n",
    "# формат: систем промпт, вопрос от пользователя, ответ из этого датасета, этот формат подается в модель в виде текста, модель просто учится генерировать такие же тексты\n",
    "\n",
    "def chatml_format_sft(example): # просто текст (для SFT)\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": example['prompt']},\n",
    "                {\"role\": \"assistant\", \"content\": example['reasoning'] + \"\\n\\n\" + example['answer']}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return {\n",
    "        \"text\": prompt\n",
    "    }"
   ],
   "metadata": {
    "id": "wBk7lT-iG0hM"
   },
   "id": "wBk7lT-iG0hM",
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def chatml_format(example): # для всех других моделей, которые не SFT\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": example['prompt']}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt\n",
    "    }"
   ],
   "metadata": {
    "id": "uMc4agmGdfUT"
   },
   "id": "uMc4agmGdfUT",
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Функции вознаграждения:"
   ],
   "metadata": {
    "id": "u4nGf5-kdkYd"
   },
   "id": "u4nGf5-kdkYd"
  },
  {
   "cell_type": "code",
   "source": [
    "# пишем эти функции с оглядкой на датасет (ответы в нем)\n",
    "\n",
    "# эта функция просто проверяет формат, она ищет есть ли такой формат в ответе\n",
    "def format_accuracy_func(completions, **kwargs):\n",
    "    rewards = []\n",
    "    for response in completions:\n",
    "        if re.search(r'```python\\n(?!.*# your (?:final )?code here)([\\s\\S]*?)\\n```', response): # просто ищем такую строчку\n",
    "            rewards.append(1.0)\n",
    "\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# эта функция проверяет правильность ответа, в нашем случае то, что код работает\n",
    "def answer_accuracy_func(completions, **kwargs):\n",
    "    rewards = []\n",
    "    for response in completions:\n",
    "        code_block = re.search(r'```python\\n(?!.*# your (?:final )?code here)([\\s\\S]*?)\\n```', response, re.DOTALL)\n",
    "\n",
    "        if code_block is not None:\n",
    "            code = code_block.group(1).strip()\n",
    "            try:\n",
    "                ast.parse(code)\n",
    "                rewards.append(1.0)\n",
    "            except SyntaxError:\n",
    "                rewards.append(0.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    return rewards"
   ],
   "metadata": {
    "id": "PqhriE6PdhHm"
   },
   "id": "PqhriE6PdhHm",
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Датасет:"
   ],
   "metadata": {
    "id": "kADCDUJCP_WE"
   },
   "id": "kADCDUJCP_WE"
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"notbadai/python_functions_reasoning\", \"default\")['train']"
   ],
   "metadata": {
    "id": "YWdQSpjRG5SY"
   },
   "id": "YWdQSpjRG5SY",
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train = dataset.select(range(100)).map(chatml_format)\n",
    "test = dataset.select(range(100, 200)).map(chatml_format)\n",
    "sft_dataset = dataset.select(range(500, 1000))\n",
    "sft_dataset = sft_dataset.map(chatml_format_sft, remove_columns=sft_dataset.column_names)"
   ],
   "metadata": {
    "id": "XygZa0ggQBeT"
   },
   "id": "XygZa0ggQBeT",
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Базовая модель:"
   ],
   "metadata": {
    "id": "7bq7vTJGY0ph"
   },
   "id": "7bq7vTJGY0ph"
  },
  {
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True\n",
    ")"
   ],
   "metadata": {
    "id": "ScdCujaTSONE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4ccba98a-112f-4edf-9e8d-fc7d7cc882e5"
   },
   "id": "ScdCujaTSONE",
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = model.to(device)"
   ],
   "metadata": {
    "id": "0Xpdp_KKpKnt"
   },
   "id": "0Xpdp_KKpKnt",
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch = test"
   ],
   "metadata": {
    "id": "9wp1Ae6kbqth"
   },
   "id": "9wp1Ae6kbqth",
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer.batch_encode_plus(batch['prompt'], return_tensors='pt', padding=True)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids['input_ids'].to(device), attention_mask=input_ids['attention_mask'].to(device),\n",
    "    max_new_tokens=400, do_sample=True, temperature=1.5, pad_token_id=tokenizer.eos_token_id #**gen_kwargs\n",
    ")"
   ],
   "metadata": {
    "id": "8Z1umsdzSOKx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a0771f81-3bc3-4db6-cd98-f35d50ca8bd4"
   },
   "id": "8Z1umsdzSOKx",
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "completions_base_model = tokenizer.batch_decode(output, skip_special_tokens=False)"
   ],
   "metadata": {
    "id": "eBkUWx5bSN-s"
   },
   "id": "eBkUWx5bSN-s",
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "completions_base_model"
   ],
   "metadata": {
    "id": "LhKrLXE5d6S-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "outputId": "a785bca0-37f0-45bd-e6d5-8bca683fb4ab"
   },
   "id": "LhKrLXE5d6S-",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Сумма по функции, которая проверяет формат: {sum(format_accuracy_func(completions_base_model))}\")"
   ],
   "metadata": {
    "id": "Q9E0oTNyd8Vo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d4312ad9-c0bc-4199-cc98-7131b6ceb199"
   },
   "id": "Q9E0oTNyd8Vo",
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Сумма по функции, которая проверяет формат: 65.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Сумма по функции, которая проверяет, работает ли код: {sum(answer_accuracy_func(completions_base_model))}\")"
   ],
   "metadata": {
    "id": "T2qYOlJ2d_-h",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c751b6fc-386c-4cb6-9d3b-5413f73b95b4"
   },
   "id": "T2qYOlJ2d_-h",
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Сумма по функции, которая проверяет, работает ли код: 59.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SFT:"
   ],
   "metadata": {
    "id": "z_2w6aJ7vKEf"
   },
   "id": "z_2w6aJ7vKEf"
  },
  {
   "cell_type": "code",
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\"\n",
    ")"
   ],
   "metadata": {
    "id": "to-MxI5METM7"
   },
   "id": "to-MxI5METM7",
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = get_peft_model(model, peft_config)"
   ],
   "metadata": {
    "id": "L2lccGfPEgsE"
   },
   "id": "L2lccGfPEgsE",
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sft_dataset[0]['text']"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "Gba0UmubzeWm",
    "outputId": "052adcf3-8db6-48f2-94cd-88a6cc305dfa"
   },
   "id": "Gba0UmubzeWm",
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n\\nYour task is to write correct and efficient python functions. Before outputting the result, give step-by-step reasoning. Always wrap the final code in markdown code blocks like this:\\n```python\\n# your final code here\\n```\\n<|im_end|>\\n<|im_start|>user\\n```python\\ndef find_missing_positive_integer(sequence):\\n    \"\"\"Write a function to find the smallest positive integer that does not occur in a given sequence of integers. The function should efficiently handle sequences with both positive and negative integers, as well as sequences with duplicates.\\n    \"\"\"\\n```<|im_end|>\\n<|im_start|>assistant\\nTo solve this problem, we need to find the smallest positive integer that is not present in a given sequence of integers. Here is a step-by-step plan:\\n\\n1. **Understand the Problem**:\\n   - The sequence contains both positive and negative integers.\\n   - There might be duplicates in the sequence.\\n   - We need to find the smallest positive integer that is not in the sequence.\\n\\n2. **Edge Cases**:\\n   - If the sequence is empty, the smallest positive integer is 1.\\n   - If the sequence contains only negative integers and zero, the smallest positive integer is 1.\\n   - If the sequence contains all positive integers starting from 1, the next integer would be the smallest missing positive integer.\\n\\n3. **Approach**:\\n   - Use a set to store the positive integers from the sequence. Sets provide O(1) average-time complexity for lookups.\\n   - Iterate through positive integers starting from 1 and check if they are in the set.\\n   - The first integer not found in the set is the smallest missing positive integer.\\n\\n4. **Implementation**:\\n   - Initialize an empty set.\\n   - Iterate through the sequence and add positive integers to the set.\\n   - Iterate through positive integers starting from 1 and check for the missing integer.\\n   - Return the first missing positive integer found.\\n\\n```python\\n    if not sequence:\\n        return 1\\n\\n    positive_integers = set()\\n    for num in sequence:\\n        if num > 0:\\n            positive_integers.add(num)\\n\\n    i = 1\\n    while i in positive_integers:\\n        i += 1\\n\\n    return i\\n```</s><|im_end|>\\n<|im_start|>assistant\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 49
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "training_args = SFTConfig(\n",
    "    max_length=200,\n",
    "    report_to=\"none\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10\n",
    ")"
   ],
   "metadata": {
    "id": "puPVUXbqvMYN"
   },
   "id": "puPVUXbqvMYN",
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=sft_dataset,\n",
    "    args=training_args\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G22Yg4tHwwOO",
    "outputId": "4faa060e-b5dd-43be-950a-f67c4bbed2b2"
   },
   "id": "G22Yg4tHwwOO",
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xLV0Z6T2wzg9",
    "outputId": "a75ab6b7-254a-4bad-8678-86aecda1b480"
   },
   "id": "xLV0Z6T2wzg9",
   "execution_count": 58,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 07:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.582900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.857700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.769100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.746700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.683200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.646300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.629900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.648600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.674700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.649800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.605300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.649900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.618400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.655400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.7587793947855631, metrics={'train_runtime': 434.4703, 'train_samples_per_second': 3.452, 'train_steps_per_second': 0.863, 'total_flos': 675890150400000.0, 'train_loss': 0.7587793947855631})"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.save_model(\"Qwen/Qwen2-0.5B-sft\")"
   ],
   "metadata": {
    "id": "opdQv3vjw2Vi"
   },
   "id": "opdQv3vjw2Vi",
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B-sft\"\n",
    "# device = 'cuda'"
   ],
   "metadata": {
    "id": "Sv7xYwfEw4ZG"
   },
   "id": "Sv7xYwfEw4ZG",
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True\n",
    ").to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQxolodA5vPz",
    "outputId": "bcc34149-5066-4890-dee2-9459038045e7"
   },
   "id": "vQxolodA5vPz",
   "execution_count": 61,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ],
   "metadata": {
    "id": "OpJRRPeN86b5"
   },
   "id": "OpJRRPeN86b5",
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer.batch_encode_plus(batch['prompt'], return_tensors='pt', padding=True)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids['input_ids'].to(device), attention_mask=input_ids['attention_mask'].to(device),\n",
    "    max_new_tokens=400, do_sample=True, temperature=1.5, pad_token_id=tokenizer.eos_token_id #**gen_kwargs\n",
    ")"
   ],
   "metadata": {
    "id": "tbqooIMU6KB2"
   },
   "id": "tbqooIMU6KB2",
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "completions_sft_model = tokenizer.batch_decode(output, skip_special_tokens=False)"
   ],
   "metadata": {
    "id": "r2GdtAOR6OFn"
   },
   "id": "r2GdtAOR6OFn",
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Сумма по функции, которая проверяет формат: {sum(format_accuracy_func(completions_sft_model))}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTeIjH9o6Ruf",
    "outputId": "24c0af28-82bb-4216-f236-630577d5e5aa"
   },
   "id": "DTeIjH9o6Ruf",
   "execution_count": 65,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Сумма по функции, которая проверяет формат: 49.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Сумма по функции, которая проверяет, работает ли код: {sum(answer_accuracy_func(completions_sft_model))}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trAzHNQm6VPK",
    "outputId": "944053ab-7944-402f-bf38-60a763c52cab"
   },
   "id": "trAzHNQm6VPK",
   "execution_count": 66,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Сумма по функции, которая проверяет, работает ли код: 49.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GRPO:"
   ],
   "metadata": {
    "id": "60Dds8-_Gs-A"
   },
   "id": "60Dds8-_Gs-A"
  },
  {
   "cell_type": "code",
   "source": [
    "training_args = GRPOConfig(output_dir=\"Qwen/Qwen2-0.5B-sft-grpo\",\n",
    "                           logging_steps=10,\n",
    "                           report_to=\"none\",\n",
    "                           num_generations=8, # сколько примеров в каждой группе GRPO, рекомендуется его меньше 8 не ставить\n",
    "                           num_train_epochs=1,\n",
    "                           temperature=1.5,\n",
    "                           label_names=[\"labels\"])"
   ],
   "metadata": {
    "id": "BunffW3gGvRZ"
   },
   "id": "BunffW3gGvRZ",
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[answer_accuracy_func, format_accuracy_func],\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    peft_config=peft_config\n",
    ")"
   ],
   "metadata": {
    "id": "IKkSF-epG8Hf"
   },
   "id": "IKkSF-epG8Hf",
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "ZLrZ7VkRHA-V",
    "outputId": "f27314a6-f2bb-4d07-c568-d9e089c6508c"
   },
   "id": "ZLrZ7VkRHA-V",
   "execution_count": 69,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 51:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.038100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.028458391427993775, metrics={'train_runtime': 3113.5814, 'train_samples_per_second': 0.032, 'train_steps_per_second': 0.032, 'total_flos': 0.0, 'train_loss': 0.028458391427993775})"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.save_model('Qwen/Qwen2-0.5B-sft-grpo')"
   ],
   "metadata": {
    "id": "FgOAgd14HEmZ"
   },
   "id": "FgOAgd14HEmZ",
   "execution_count": 70,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-sft-grpo\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ],
   "metadata": {
    "id": "nBrQgS_TH3rL"
   },
   "id": "nBrQgS_TH3rL",
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_ref = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-sft\", device_map='cuda', torch_dtype=torch.bfloat16)\n",
    "\n",
    "device = model_ref.device"
   ],
   "metadata": {
    "id": "iHhTsrdLHRF-"
   },
   "id": "iHhTsrdLHRF-",
   "execution_count": 73,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = PeftModel.from_pretrained(model_ref, \"Qwen/Qwen2-0.5B-sft-grpo\")"
   ],
   "metadata": {
    "id": "OJg8WG7bHHO_"
   },
   "id": "OJg8WG7bHHO_",
   "execution_count": 74,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer.batch_encode_plus(batch['prompt'], return_tensors='pt', padding=True)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids['input_ids'].to(device), attention_mask=input_ids['attention_mask'].to(device),\n",
    "    max_new_tokens=400, do_sample=True, temperature=1.5, pad_token_id=tokenizer.eos_token_id #**gen_kwargs\n",
    ")"
   ],
   "metadata": {
    "id": "AhE-59ShHcoi"
   },
   "id": "AhE-59ShHcoi",
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "completions_grpo_model = tokenizer.batch_decode(output, skip_special_tokens=False)"
   ],
   "metadata": {
    "id": "AzAxKHh2IDc4"
   },
   "id": "AzAxKHh2IDc4",
   "execution_count": 76,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Сумма по функции, которая проверяет формат: {sum(format_accuracy_func(completions_grpo_model))}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWXDxdwlIGTy",
    "outputId": "42a0af16-5b32-4fd8-bde6-0f25f75779e4"
   },
   "id": "HWXDxdwlIGTy",
   "execution_count": 77,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Сумма по функции, которая проверяет формат: 77.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Сумма по функции, которая проверяет, работает ли код: {sum(answer_accuracy_func(completions_grpo_model))}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rtXGvctqIJfe",
    "outputId": "1d336c8f-3a55-4e02-866c-df08d1783cc0"
   },
   "id": "rtXGvctqIJfe",
   "execution_count": 78,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Сумма по функции, которая проверяет, работает ли код: 74.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Вывод"
   ],
   "metadata": {
    "id": "NznwUzxUcpf5"
   },
   "id": "NznwUzxUcpf5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Базовая модель показывает вполне неплохие результаты (и в completions видно, что модель даже пытается следовать промпту). После дообучения (sft) качество хуже, что странно (оно ниже базовой модели). Но GRPO модель в любом случае показала самые лучшие результаты"
   ],
   "metadata": {
    "id": "lHGi0-2bcsUp"
   },
   "id": "lHGi0-2bcsUp"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
