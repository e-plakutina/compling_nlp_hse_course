{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43fc8290",
   "metadata": {},
   "source": [
    "## Задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a5ace",
   "metadata": {},
   "source": [
    "Посчитайте частоты для 5-грамм в корпусе lenta.txt. двумя способами:  \n",
    "1) lenta.txt -> sent_tokenize (russian) -> word_tokenize -> ngrammer  \n",
    "2) lenta.txt -> word_tokene(preserve_line=True) - ngrammer  \n",
    "    \n",
    "Проанализируйте топ-20 самых частотных нграмм и проверьте есть ли различия? "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T11:29:39.121983Z",
     "start_time": "2024-09-26T11:29:38.886431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !mkdir data\n",
    "# !wget https://github.com/mannefedov/compling_nlp_hse_course/raw/master/data/lenta.txt.zip -P data\n",
    "# !unzip -o data/lenta.txt.zip -d data/\n",
    "\n",
    "# открываем файл с текстом\n",
    "import codecs\n",
    "\n",
    "from gensim.models.phrases import npmi_scorer\n",
    "\n",
    "file = codecs.open('data/lenta.txt', 'r', 'utf_8_sig')\n",
    "corpus = file.read()\n",
    "file.close()\n",
    "\n",
    "# Импортируем nltk\n",
    "# !pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Импортируем стоп-слова из nltk, чтобы вывести самые частотные смысловые слова\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "\n",
    "import re\n",
    "\n",
    "# Подсчет количества вхождений каждого слова\n",
    "from collections import Counter"
   ],
   "id": "c5d0e5d6eb7f5332",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\eliza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eliza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Первый способ решения:",
   "id": "abb8e7fc28968be"
  },
  {
   "cell_type": "code",
   "id": "957f5656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T12:16:35.777331Z",
     "start_time": "2024-09-26T12:16:24.323739Z"
    }
   },
   "source": [
    "# Токенайзер:\n",
    "sentences = sent_tokenize(corpus, language='russian')\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Продолжение разбиения на токены, только теперь не учитывается регистр:\n",
    "tokenized_sentences = [[token.lower() for token in sentence if not re.match('\\W+', token)]\n",
    "                       for sentence in tokenized_sentences]\n",
    "\n",
    "# Выводим 5-граммы через ngrammer:\n",
    "def ngrammer(tokens, n=5):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "# Список н-граммов с удалением из них стоп-слов:\n",
    "fivegram_counts = Counter()\n",
    "for sentence in tokenized_sentences:\n",
    "    fivegram_counts.update(ngrammer([token for token in sentence if token not in russian_stopwords], 5))\n",
    "\n",
    "print(\"Список н-граммов с удалением из них стоп-слов:\", fivegram_counts.most_common(20), \"\\n\")\n",
    "\n",
    "# Список н-граммов с включенными в них стоп-словами:\n",
    "fivegram_counts = Counter()\n",
    "for sentence in tokenized_sentences:\n",
    "    fivegram_counts.update(ngrammer([token for token in sentence], 5))\n",
    "\n",
    "print(\"Список н-граммов без удаления из них стоп-слов:\", fivegram_counts.most_common(20))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\eliza\\AppData\\Local\\Temp\\ipykernel_30284\\1529925989.py:6: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  tokenized_sentences = [[token.lower() for token in sentence if not re.match('\\W+', token)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Список н-граммов с удалением из них стоп-слов: [('объединенной группировки войск северном кавказе', 83), ('сообщает риа новости ссылкой пресс-службу', 57), ('обязанности президента россии владимир путин', 47), ('группировки федеральных сил северном кавказе', 37), ('помощник президента россии сергей ястржембский', 35), ('делам печати телерадиовещания средств массовых', 31), ('объединенной группировки федеральных сил северном', 31), ('печати телерадиовещания средств массовых коммуникаций', 30), ('штабе объединенной группировки федеральных сил', 25), ('министр иностранных дел россии игорь', 24), ('первый заместитель начальника генерального штаба', 23), ('сообщает риа новости ссылкой источники', 23), ('объединенной группировкой войск северном кавказе', 23), ('иностранных дел россии игорь иванов', 23), ('прямом эфире радиостанции эхо москвы', 20), ('главного управления борьбе организованной преступностью', 20), ('начальника генерального штаба вооруженных сил', 20), ('заместитель начальника генштаба вооруженных сил', 20), ('сообщает федеральное агентство новостей ссылкой', 20), ('исполняющий обязанности президента россии владимир', 20)] \n",
      "\n",
      "Список н-граммов без удаления из них стоп-слов: [('риа новости со ссылкой на', 400), ('сообщает риа новости со ссылкой', 320), ('как сообщили риа новости в', 196), ('как сообщает риа новости со', 149), ('сообщает интерфакс со ссылкой на', 142), ('сообщает итар-тасс со ссылкой на', 118), ('об этом риа новости сообщили', 113), ('об этом сообщает риа новости', 104), ('этом риа новости сообщили в', 99), ('со ссылкой на источники в', 93), ('сообщили риа новости в пресс-службе', 88), ('группировки войск на северном кавказе', 84), ('как сообщает интерфакс со ссылкой', 83), ('объединенной группировки войск на северном', 83), ('новости со ссылкой на пресс-службу', 76), ('эхо москвы со ссылкой на', 76), ('этом сообщает риа новости со', 75), ('в связи с тем что', 70), ('по борьбе с организованной преступностью', 66), ('как сообщает итар-тасс со ссылкой', 58)]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Второй способ решения:",
   "id": "c74b88c81eaf9e49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T12:17:48.259783Z",
     "start_time": "2024-09-26T12:17:39.831496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Токенайзер:\n",
    "tokenized_lines = [word_tokenize(line, preserve_line=True) for line in corpus.splitlines()]\n",
    "\n",
    "# Продолжение разбиения на токены, только теперь не учитывается регистр:\n",
    "tokenized_lines = [[token.lower() for token in line if not re.match('\\W+', token)]\n",
    "                   for line in tokenized_lines]\n",
    "\n",
    "# Выводим 5-граммы через ngrammer:\n",
    "def ngrammer(tokens, n=5):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "# Список н-граммов с удалением из них стоп-слов:\n",
    "fivegram_counts = Counter()\n",
    "for line in tokenized_lines:\n",
    "    fivegram_counts.update(ngrammer([token for token in line if token not in russian_stopwords]))\n",
    "    \n",
    "print(\"Список н-граммов с удалением из них стоп-слов:\", fivegram_counts.most_common(20), \"\\n\")\n",
    "\n",
    "# Список н-граммов с включенными в них стоп-словами:\n",
    "fivegram_counts = Counter()\n",
    "for line in tokenized_lines:\n",
    "    fivegram_counts.update(ngrammer([token for token in line], 5))\n",
    "\n",
    "print(\"Список н-граммов без удаления из них стоп-слов:\", fivegram_counts.most_common(20))"
   ],
   "id": "48ad6bbeebef57ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\eliza\\AppData\\Local\\Temp\\ipykernel_30284\\910077497.py:5: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  tokenized_lines = [[token.lower() for token in line if not re.match('\\W+', token)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Список н-граммов с удалением из них стоп-слов: [('сообщает риа новости ссылкой пресс-службу', 57), ('объединенной группировки войск северном кавказе', 56), ('обязанности президента россии владимир путин', 46), ('делам печати телерадиовещания средств массовых', 31), ('объединенной группировки федеральных сил северном', 31), ('печати телерадиовещания средств массовых коммуникаций', 29), ('объединенной группировки войск северном кавказе.', 27), ('министр иностранных дел россии игорь', 24), ('штабе объединенной группировки федеральных сил', 24), ('первый заместитель начальника генерального штаба', 23), ('сообщает риа новости ссылкой источники', 23), ('объединенной группировкой войск северном кавказе', 23), ('помощник президента россии сергей ястржембский', 21), ('прямом эфире радиостанции эхо москвы', 20), ('начальника генерального штаба вооруженных сил', 20), ('группировки федеральных сил северном кавказе', 20), ('заместитель начальника генштаба вооруженных сил', 20), ('сообщает федеральное агентство новостей ссылкой', 20), ('исполняющий обязанности президента россии владимир', 20), ('главного управления борьбе организованной преступностью', 19)] \n",
      "\n",
      "Список н-граммов без удаления из них стоп-слов: [('риа новости со ссылкой на', 400), ('сообщает риа новости со ссылкой', 320), ('как сообщили риа новости в', 196), ('как сообщает риа новости со', 149), ('сообщает интерфакс со ссылкой на', 142), ('сообщает итар-тасс со ссылкой на', 118), ('об этом риа новости сообщили', 113), ('об этом сообщает риа новости', 104), ('этом риа новости сообщили в', 99), ('со ссылкой на источники в', 93), ('сообщили риа новости в пресс-службе', 88), ('как сообщает интерфакс со ссылкой', 83), ('объединенной группировки войск на северном', 83), ('эхо москвы со ссылкой на', 77), ('новости со ссылкой на пресс-службу', 76), ('этом сообщает риа новости со', 75), ('в связи с тем что', 70), ('как сообщает итар-тасс со ссылкой', 58), ('группировки войск на северном кавказе', 57), ('по борьбе с организованной преступностью', 55)]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Анализ результатов:\n",
    "Оба способа выдают примерно одинаковые результаты, однако первый способ с токенизацией по предложениям сохраняет смысловую связность н-граммов, так как в этом способе не происходит разрыва предложений на границах строк и контекст ограничен предложениями. Из-за этого частотность в первом способе выше, чем во втором. Например, если удалять из результатов стоп-слова, фраза \"объединенной группировки войск северном кавказе\" встречается 83 раза в первом подходе, а во втором — 56 раз. Также при использовании второго подхода возникают случаи, когда в н-грамм попадают знаки препинания. Например, в 5-грамме \"объединенной группировки войск северном кавказе.\" добавляется точка в конце фразы, так как строка могла заканчиваться на этой 5-грамме. Для н-грамм без удаления стоп-слов, топовые фразы идентичны в обоих подходах, например, \"риа новости со ссылкой на\" встречается 400 раз в обоих случаях. При удалении стоп-слов разница становится заметнее: первый способ создает более связные фразы, в то время как второй метод может их разбивать (например, \"объединенной группировки войск северном кавказе\" — 83 в первом способе, 56 во втором). "
   ],
   "id": "f639bce6b642a4c0"
  },
  {
   "cell_type": "markdown",
   "id": "b5781f34",
   "metadata": {},
   "source": [
    "## Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292716e",
   "metadata": {},
   "source": [
    "Найдите какую-то инетересную (по вашему мнению) закономерность на https://books.google.com/ngrams/ для русского языка (с 1990 по 2022)\n",
    "\n",
    "Вставьте сюда скриншот"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Закономерность использования биграмма \"глобальное потепление\":](photo_2024-09-26_16-43-38.png)",
   "id": "eaca73bfd48b8f8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Биграмм \"глобальное потепление\" стал использоваться чаще, приобретя популярность в 2009, следовательно, у людей повысился интерес к этой теме и она чаще начала освещаться в сми",
   "id": "8dab2b12610a7a8d"
  },
  {
   "cell_type": "markdown",
   "id": "8a0a89ec",
   "metadata": {},
   "source": [
    "## Заданиe 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c35e9",
   "metadata": {},
   "source": [
    "Когда мы разбирали PMI мы использовали такую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "id": "221f1bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:13:00.596574Z",
     "start_time": "2024-09-26T18:13:00.577273Z"
    }
   },
   "source": [
    "def scorer_simple(word_count_a, word_count_b, bigram_count, *args):\n",
    "    try:\n",
    "        score = bigram_count/((word_count_a+word_count_b))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return score"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "53fd2def",
   "metadata": {},
   "source": [
    "Но если вы посмотрите на определение в википедии, то увидите, что формула немного другая ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/094243d23c19d2d032f6bb26c4dc4f47d98d32f8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1905862",
   "metadata": {},
   "source": [
    "Перепишите функцию, чтобы она точно соответствовала этому определению. Расчитайте PMI для всех биграммов также как мы делали в семинаре с помощью функции score_bigrams используя изначальный scorer и обновленный. Посмотрите есть ли разница в топ-10 биграммов. Подумайте почему результаты совпадают/отличаются?\n",
    "\n",
    "*Подсказка: для вероятностей можно поделить на количество слов в корпусе"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Обновленный scorer:",
   "id": "52f92bb25836dab6"
  },
  {
   "cell_type": "code",
   "id": "1431f618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:52:36.481097Z",
     "start_time": "2024-09-26T18:52:36.464989Z"
    }
   },
   "source": [
    "import math\n",
    "\n",
    "def scorer_pmi(word_count_a, word_count_b, bigram_count, total_words):\n",
    "    try:\n",
    "        # Вероятности появления слов отдельно\n",
    "        p_a = word_count_a / total_words\n",
    "        p_b = word_count_b / total_words\n",
    "        \n",
    "        # Вероятность появления биграммов\n",
    "        p_ab = bigram_count / total_words\n",
    "        \n",
    "        pmi = math.log(p_ab / (p_a * p_b), 2)\n",
    "        \n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return pmi"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Общий код для двух scorer:",
   "id": "d01dccff65ac3be7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:22:28.105823Z",
     "start_time": "2024-09-26T18:22:28.090485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ngrammer с добавленными стоп-словами:\n",
    "def ngrammer(tokens, n=2, stops=set()):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens if token not in stops]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "# Сбор статистики по тексту:\n",
    "def collect_stats(corpus, stops):\n",
    "\n",
    "    unigrams = Counter()\n",
    "    bigrams = Counter()\n",
    "    \n",
    "    for sent in corpus:\n",
    "        unigrams.update(sent)\n",
    "        bigrams.update(ngrammer(sent, 2, stops))\n",
    "    \n",
    "    return unigrams, bigrams"
   ],
   "id": "80205c1e8f8631e4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Топ-10 биграммов с использованием изначального scorer (scorer_simple):",
   "id": "acd68f6663f2007"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:22:34.674510Z",
     "start_time": "2024-09-26T18:22:32.522794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Метрика для биграмма:\n",
    "def score_bigrams(unigrams, bigrams, scorer, threshold=-100000):\n",
    "    bigram2score = Counter()\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        word_a, word_b = bigram.split()\n",
    "        score = scorer(unigrams[word_a], unigrams[word_b], \n",
    "                       bigrams[bigram])\n",
    "        \n",
    "        # если метрика выше порога, добавляем в словарик\n",
    "        if score > threshold:\n",
    "            bigram2score[bigram] = score\n",
    "    \n",
    "    return bigram2score\n",
    "\n",
    "\n",
    "\n",
    "unigrams, bigrams = collect_stats(tokenized_sentences, russian_stopwords)\n",
    "\n",
    "bigram2score = score_bigrams(unigrams, bigrams, scorer_simple)\n",
    "\n",
    "\n",
    "bigram2score.most_common(10)"
   ],
   "id": "53817d0efa40c81e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('сопоцкина друскеник', 0.5),\n",
       " ('неприятель приблизившись', 0.5),\n",
       " ('саноку обстреливалась', 0.5),\n",
       " ('м.ю лермонтова', 0.5),\n",
       " ('австрийский аэроплан', 0.5),\n",
       " ('показывался аэроплан-птица', 0.5),\n",
       " ('das ist', 0.5),\n",
       " ('ist nesteroff', 0.5),\n",
       " ('песнь нестерове', 0.5),\n",
       " ('могучий унесся', 0.5)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Топ-10 биграммов с использованием измененного scorer (scorer_pmi):",
   "id": "9e088121401ca6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:56:53.848352Z",
     "start_time": "2024-09-26T18:56:51.205603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Метрика для биграмма:\n",
    "def score_bigrams_2(unigrams, bigrams, scorer, total_words, threshold=-100000):\n",
    "    bigram2score = Counter()\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        word_a, word_b = bigram.split()\n",
    "        score = scorer(unigrams[word_a], unigrams[word_b], \n",
    "                       bigrams[bigram], total_words)\n",
    "        \n",
    "        # если метрика выше порога, добавляем в словарик\n",
    "        if score > threshold:\n",
    "            bigram2score[bigram] = score\n",
    "    \n",
    "    return bigram2score\n",
    "\n",
    "\n",
    "\n",
    "unigrams, bigrams = collect_stats(tokenized_sentences, russian_stopwords)\n",
    "\n",
    "total_words = sum(unigrams.values())\n",
    "\n",
    "bigram2score = score_bigrams_2(unigrams, bigrams, scorer_pmi, total_words)\n",
    "\n",
    "\n",
    "bigram2score.most_common(10)"
   ],
   "id": "bdf21be18377e68",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('сопоцкина друскеник', 20.510568127854185),\n",
       " ('неприятель приблизившись', 20.510568127854185),\n",
       " ('саноку обстреливалась', 20.510568127854185),\n",
       " ('м.ю лермонтова', 20.510568127854185),\n",
       " ('австрийский аэроплан', 20.510568127854185),\n",
       " ('показывался аэроплан-птица', 20.510568127854185),\n",
       " ('das ist', 20.510568127854185),\n",
       " ('ist nesteroff', 20.510568127854185),\n",
       " ('песнь нестерове', 20.510568127854185),\n",
       " ('могучий унесся', 20.510568127854185)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Топ-10 биграмм совпадает при использовании обеих функций, потому что обе метрики выделяют биграммы, где слова часто встречаются вместе, независимо от того, как часто они встречаются по отдельности. Однако результаты функций различаются по значениям метрик (простая вероятность, pmi).",
   "id": "c29d50f25fdaf564"
  },
  {
   "cell_type": "markdown",
   "id": "0a6e1c99",
   "metadata": {},
   "source": [
    "## Задание 4*\n",
    "\n",
    "Обновите функцию получившуюся в предыдущем задании так, чтобы вместо произведения/деления вероятностей использовались сложение и вычитание логирифмов. "
   ]
  },
  {
   "cell_type": "code",
   "id": "3f55a362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T20:49:36.709377Z",
     "start_time": "2024-09-26T20:49:36.688972Z"
    }
   },
   "source": [
    "def scorer_log(word_count_a, word_count_b, bigram_count, total_words):\n",
    "    try:\n",
    "        # Логарифмы вероятностей появления слов и биграмм\n",
    "        log_p_a = math.log(word_count_a / total_words, 2)\n",
    "        log_p_b = math.log(word_count_b / total_words, 2)\n",
    "        log_p_ab = math.log(bigram_count / total_words, 2)\n",
    "        \n",
    "        # Логарифм PMI: log(p(x, y)) - log(p(x)) - log(p(y))\n",
    "        pmi = log_p_ab - log_p_a - log_p_b\n",
    "        \n",
    "    except (ZeroDivisionError, ValueError):\n",
    "        return 0\n",
    "    \n",
    "    return pmi"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "b22785f4",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1121e53",
   "metadata": {},
   "source": [
    "Исследуйте gensim.models.Phrases. Проверьте сколько дефолтных scoring функций есть в этом классе. Попробуйте все доступные по умолчанию scoring функции и попробуйте настраивать для них значение threshold и min_count. Попробуйте сделать так, чтобы собиралось как можно больше нграммов. Попробуйте строить последовательность gensim.models.Phrases, чтобы строить более длинные нграммы"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T20:57:50.265804Z",
     "start_time": "2024-09-26T20:57:50.246668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!pip install gensim\n",
    "import gensim\n",
    "\n",
    "def scorer_w2v(worda_count, wordb_count, bigram_count, corpus_word_count, len_vocab=0,  min_count=0):\n",
    "\n",
    "    try:\n",
    "        score = ((bigram_count - min_count) * corpus_word_count) / (worda_count * wordb_count)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score"
   ],
   "id": "130389e0dcd2ebf8",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "716fba84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T21:27:42.234490Z",
     "start_time": "2024-09-26T21:27:32.639962Z"
    }
   },
   "source": [
    "# собираем статистики\n",
    "ph = gensim.models.Phrases(tokenized_sentences, \n",
    "                           min_count=1, \n",
    "                           threshold=1.,\n",
    "                           scoring=scorer_w2v)\n",
    "\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "\n",
    "ph2 = gensim.models.Phrases(p[tokenized_sentences],  \n",
    "                            min_count=1, \n",
    "                            threshold=1., \n",
    "                            scoring=scorer_w2v)\n",
    "\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "\n",
    "p2[p[tokenized_sentences[3]]]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['с_раннего_утра',\n",
       " '14_сентября',\n",
       " 'огонь',\n",
       " 'достиг',\n",
       " 'значительного',\n",
       " 'напряжения']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T21:32:31.310669Z",
     "start_time": "2024-09-26T21:32:31.298024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Нграммы для первых 20 предложений по настройкам из исходного кода:\n",
    "for sentence in tokenized_sentences[:20]:\n",
    "    print(p2[p[sentence]])"
   ],
   "id": "d5d8953cd9d39669",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['бои', 'у', 'сопоцкина', 'и', 'друскеник', 'закончились', 'отступлением', 'германцев']\n",
      "['неприятель', 'приблизившись', 'с_севера', 'к', 'осовцу', 'начал', 'артиллерийскую', 'борьбу_с', 'крепостью']\n",
      "['в', 'артиллерийском', 'бою', 'принимают_участие', 'тяжелые', 'калибры']\n",
      "['с_раннего_утра', '14_сентября', 'огонь', 'достиг', 'значительного', 'напряжения']\n",
      "['попытка', 'германской', 'пехоты', 'пробиться', 'ближе_к', 'крепости', 'отражена']\n",
      "['в_галиции', 'мы', 'заняли', 'дембицу']\n",
      "['большая', 'колонна', 'отступавшая', 'по_шоссе', 'от', 'перемышля', 'к', 'саноку', 'обстреливалась', 'с_высот', 'нашей', 'батареей', 'и', 'бежала', 'бросив', 'парки', 'обоз', 'и_автомобили']\n",
      "['вылазки', 'гарнизона', 'перемышля', 'остаются', 'безуспешными']\n",
      "['при', 'продолжающемся', 'отступлении', 'австрийцев', 'обнаруживается', 'полное', 'перемешивание', 'их_частей', 'захватываются', 'новые', 'партии', 'пленных', 'орудия', 'и', 'прочая', 'материальная', 'часть']\n",
      "['на_перевале', 'ужок', 'мы', 'разбили', 'неприятельский', 'отряд', 'взяли_его', 'артиллерию_и', 'много', 'пленных_и', 'продолжая', 'преследовать', 'вступили_в', 'пределы', 'венгрии']\n",
      "['русский_инвалид_16_сентября', '1914', 'года.министерство', 'народного', 'просвещения', 'в_виду', 'происходящих', 'чрезвычайных', 'событий', 'признало', 'соответственным', 'в_день', 'годовщины_со_дня_рождения', 'м.ю', 'лермонтова', '2-го', 'октября', '1914_года', 'ограничиться', 'совершением', 'в_учебных', 'заведениях', 'панихиды_по', 'поэту', 'отложив', 'празднование', 'юбилея', 'до', 'более', 'благоприятного', 'времени']\n",
      "['русский_инвалид_16_сентября', '1914', 'года.штабс-капитан', 'п.', 'н.', 'нестеров', 'на_днях', 'увидев_в', 'районе', 'желтиева', 'в_галиции', 'летящий', 'над', 'нашим', 'расположением', 'австрийский', 'аэроплан', 'собиравшийся', 'бросить', 'бомбы', 'взлетел', 'на_воздух', 'атаковал', 'неприятеля', 'и', 'протаранил', 'неприятельский', 'аппарат', 'предотвратив', 'жертвы', 'в_наших', 'войсках']\n",
      "['сам', 'нестеров', 'при_этом', 'погиб', 'смертью', 'героя']\n",
      "['по_словам', 'доставленных', 'в_киев', 'пленных', 'австрийских', 'офицеров', 'всей', 'неприятельской', 'армии', 'хорошо_известно', 'имя', 'нестерова']\n",
      "['во_время', 'воздушных', 'разведок', 'русских', 'авиаторов', 'австрийцы', 'всегда', 'безошибочно', 'определяли', 'каким', 'аппаратом', 'управлял', 'нестеров']\n",
      "['когда', 'показывался', 'аэроплан-птица', 'красиво', 'и', 'вольно', 'паривший', 'в_воздухе', 'австрийцы', 'указывали', 'das', 'ist', 'nesteroff']\n",
      "['австрийцы', 'боялись', 'покойного', 'и_все', 'их', 'усилия', 'были_направлены', 'к_прекращению', 'его_деятельности']\n",
      "['за_задержание', 'отважного', 'летчика', 'была_объявлена', 'большая', 'премия']\n",
      "['нестеров', 'погиб_в', '27_лет']\n",
      "['после', 'нестерова', 'остались_жена', 'и_двое', 'детей', 'девочка', '5-ти_лет', 'и', 'мальчик', '3-х', 'лет']\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T21:36:07.564291Z",
     "start_time": "2024-09-26T21:35:56.847578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Изменение настроек для scorer_w2v\n",
    "ph = gensim.models.Phrases(tokenized_sentences, \n",
    "                           min_count=5, \n",
    "                           threshold=0.5,\n",
    "                           scoring=scorer_w2v)\n",
    "\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "\n",
    "ph2 = gensim.models.Phrases(p[tokenized_sentences],  \n",
    "                            min_count=5, \n",
    "                            threshold=0.5, \n",
    "                            scoring=scorer_w2v)\n",
    "\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "\n",
    "for sentence in tokenized_sentences[:20]:\n",
    "    print(p2[p[sentence]])"
   ],
   "id": "5c90d3e3a1a4baba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['бои', 'у', 'сопоцкина', 'и', 'друскеник', 'закончились', 'отступлением', 'германцев']\n",
      "['неприятель', 'приблизившись', 'с_севера', 'к', 'осовцу', 'начал', 'артиллерийскую', 'борьбу_с', 'крепостью']\n",
      "['в', 'артиллерийском', 'бою', 'принимают_участие', 'тяжелые', 'калибры']\n",
      "['с', 'раннего', 'утра', '14_сентября', 'огонь', 'достиг', 'значительного', 'напряжения']\n",
      "['попытка', 'германской', 'пехоты', 'пробиться', 'ближе_к', 'крепости', 'отражена']\n",
      "['в', 'галиции', 'мы', 'заняли', 'дембицу']\n",
      "['большая', 'колонна', 'отступавшая', 'по', 'шоссе', 'от', 'перемышля', 'к', 'саноку', 'обстреливалась', 'с', 'высот', 'нашей', 'батареей', 'и', 'бежала', 'бросив', 'парки', 'обоз', 'и', 'автомобили']\n",
      "['вылазки', 'гарнизона', 'перемышля', 'остаются', 'безуспешными']\n",
      "['при', 'продолжающемся', 'отступлении', 'австрийцев', 'обнаруживается', 'полное', 'перемешивание', 'их', 'частей', 'захватываются', 'новые', 'партии', 'пленных', 'орудия', 'и', 'прочая', 'материальная', 'часть']\n",
      "['на', 'перевале', 'ужок', 'мы', 'разбили', 'неприятельский', 'отряд', 'взяли', 'его', 'артиллерию', 'и', 'много', 'пленных', 'и', 'продолжая', 'преследовать', 'вступили_в', 'пределы', 'венгрии']\n",
      "['русский', 'инвалид', '16_сентября', '1914', 'года.министерство', 'народного', 'просвещения', 'в_виду', 'происходящих', 'чрезвычайных', 'событий', 'признало', 'соответственным', 'в_день', 'годовщины', 'со_дня', 'рождения', 'м.ю', 'лермонтова', '2-го', 'октября', '1914', 'года', 'ограничиться', 'совершением', 'в', 'учебных', 'заведениях', 'панихиды', 'по', 'поэту', 'отложив', 'празднование', 'юбилея', 'до', 'более', 'благоприятного', 'времени']\n",
      "['русский', 'инвалид', '16_сентября', '1914', 'года.штабс-капитан', 'п.', 'н.', 'нестеров', 'на_днях', 'увидев', 'в_районе', 'желтиева', 'в', 'галиции', 'летящий', 'над', 'нашим', 'расположением', 'австрийский', 'аэроплан', 'собиравшийся', 'бросить', 'бомбы', 'взлетел', 'на', 'воздух', 'атаковал', 'неприятеля', 'и', 'протаранил', 'неприятельский', 'аппарат', 'предотвратив', 'жертвы', 'в_наших', 'войсках']\n",
      "['сам', 'нестеров', 'при_этом', 'погиб', 'смертью', 'героя']\n",
      "['по_словам', 'доставленных', 'в_киев', 'пленных', 'австрийских', 'офицеров', 'всей', 'неприятельской', 'армии', 'хорошо', 'известно', 'имя', 'нестерова']\n",
      "['во_время', 'воздушных', 'разведок', 'русских', 'авиаторов', 'австрийцы', 'всегда', 'безошибочно', 'определяли', 'каким', 'аппаратом', 'управлял', 'нестеров']\n",
      "['когда', 'показывался', 'аэроплан-птица', 'красиво', 'и', 'вольно', 'паривший', 'в_воздухе', 'австрийцы', 'указывали', 'das', 'ist', 'nesteroff']\n",
      "['австрийцы', 'боялись', 'покойного', 'и_все', 'их', 'усилия', 'были_направлены', 'к', 'прекращению', 'его', 'деятельности']\n",
      "['за', 'задержание', 'отважного', 'летчика', 'была', 'объявлена', 'большая', 'премия']\n",
      "['нестеров', 'погиб_в', '27_лет']\n",
      "['после', 'нестерова', 'остались', 'жена_и', 'двое_детей', 'девочка', '5-ти', 'лет_и', 'мальчик', '3-х', 'лет']\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T21:52:52.774265Z",
     "start_time": "2024-09-26T21:52:42.202978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Дефолтный scorer:\n",
    "ph = gensim.models.Phrases(tokenized_sentences, \n",
    "                           min_count=1, \n",
    "                           threshold=0.3)\n",
    "\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "\n",
    "ph2 = gensim.models.Phrases(p[tokenized_sentences],  \n",
    "                            min_count=1, \n",
    "                            threshold=0.3)\n",
    "\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "\n",
    "for sentence in tokenized_sentences[:20]:\n",
    "    print(p2[p[sentence]])"
   ],
   "id": "8873d9f0dd978c11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['бои', 'у', 'сопоцкина', 'и', 'друскеник', 'закончились', 'отступлением', 'германцев']\n",
      "['неприятель', 'приблизившись', 'с_севера', 'к', 'осовцу', 'начал', 'артиллерийскую', 'борьбу_с', 'крепостью']\n",
      "['в', 'артиллерийском', 'бою', 'принимают_участие', 'тяжелые', 'калибры']\n",
      "['с_раннего_утра', '14_сентября', 'огонь', 'достиг', 'значительного', 'напряжения']\n",
      "['попытка', 'германской', 'пехоты', 'пробиться', 'ближе_к', 'крепости', 'отражена']\n",
      "['в_галиции', 'мы', 'заняли', 'дембицу']\n",
      "['большая', 'колонна', 'отступавшая', 'по_шоссе', 'от', 'перемышля', 'к', 'саноку', 'обстреливалась', 'с_высот', 'нашей', 'батареей', 'и', 'бежала', 'бросив', 'парки', 'обоз', 'и_автомобили']\n",
      "['вылазки', 'гарнизона', 'перемышля', 'остаются', 'безуспешными']\n",
      "['при', 'продолжающемся', 'отступлении', 'австрийцев', 'обнаруживается', 'полное', 'перемешивание', 'их_частей', 'захватываются', 'новые', 'партии', 'пленных', 'орудия', 'и', 'прочая', 'материальная', 'часть']\n",
      "['на_перевале', 'ужок', 'мы', 'разбили', 'неприятельский', 'отряд', 'взяли_его', 'артиллерию_и', 'много', 'пленных_и', 'продолжая', 'преследовать', 'вступили_в', 'пределы', 'венгрии']\n",
      "['русский_инвалид_16_сентября', '1914', 'года.министерство', 'народного', 'просвещения', 'в_виду', 'происходящих', 'чрезвычайных', 'событий', 'признало', 'соответственным', 'в_день', 'годовщины_со_дня_рождения', 'м.ю', 'лермонтова', '2-го', 'октября', '1914_года', 'ограничиться', 'совершением', 'в_учебных', 'заведениях', 'панихиды_по', 'поэту', 'отложив', 'празднование', 'юбилея', 'до', 'более', 'благоприятного', 'времени']\n",
      "['русский_инвалид_16_сентября', '1914', 'года.штабс-капитан', 'п.', 'н.', 'нестеров', 'на_днях', 'увидев_в', 'районе', 'желтиева', 'в_галиции', 'летящий', 'над', 'нашим', 'расположением', 'австрийский', 'аэроплан', 'собиравшийся', 'бросить', 'бомбы', 'взлетел', 'на_воздух', 'атаковал', 'неприятеля', 'и', 'протаранил', 'неприятельский', 'аппарат', 'предотвратив', 'жертвы', 'в_наших', 'войсках']\n",
      "['сам', 'нестеров', 'при_этом', 'погиб', 'смертью', 'героя']\n",
      "['по_словам', 'доставленных', 'в_киев', 'пленных', 'австрийских', 'офицеров', 'всей', 'неприятельской', 'армии', 'хорошо_известно', 'имя', 'нестерова']\n",
      "['во_время', 'воздушных', 'разведок', 'русских', 'авиаторов', 'австрийцы', 'всегда', 'безошибочно', 'определяли', 'каким', 'аппаратом', 'управлял', 'нестеров']\n",
      "['когда', 'показывался', 'аэроплан-птица', 'красиво', 'и', 'вольно', 'паривший', 'в_воздухе', 'австрийцы', 'указывали', 'das', 'ist', 'nesteroff']\n",
      "['австрийцы', 'боялись', 'покойного', 'и_все', 'их', 'усилия', 'были_направлены', 'к_прекращению', 'его_деятельности']\n",
      "['за_задержание', 'отважного', 'летчика', 'была_объявлена', 'большая', 'премия']\n",
      "['нестеров', 'погиб_в', '27_лет']\n",
      "['после', 'нестерова', 'остались_жена', 'и_двое', 'детей', 'девочка', '5-ти_лет', 'и', 'мальчик', '3-х', 'лет']\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T21:58:23.908742Z",
     "start_time": "2024-09-26T21:58:12.911876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# nmpi:\n",
    "ph = gensim.models.Phrases(tokenized_sentences, \n",
    "                           min_count=1,\n",
    "                           threshold=0.5,\n",
    "                           scoring='npmi')\n",
    "\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "\n",
    "ph2 = gensim.models.Phrases(p[tokenized_sentences], \n",
    "                            min_count=1, \n",
    "                            threshold=0.5,\n",
    "                            scoring='npmi')\n",
    "\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "\n",
    "for sentence in tokenized_sentences[:20]:\n",
    "    print(p2[p[sentence]])"
   ],
   "id": "3c73ac90b0b3fc50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['бои', 'у', 'сопоцкина', 'и', 'друскеник_закончились', 'отступлением_германцев']\n",
      "['неприятель_приблизившись', 'с', 'севера', 'к', 'осовцу_начал', 'артиллерийскую_борьбу', 'с', 'крепостью']\n",
      "['в', 'артиллерийском_бою', 'принимают_участие', 'тяжелые_калибры']\n",
      "['с', 'раннего_утра', '14', 'сентября', 'огонь', 'достиг_значительного_напряжения']\n",
      "['попытка_германской_пехоты', 'пробиться_ближе', 'к', 'крепости_отражена']\n",
      "['в', 'галиции_мы', 'заняли_дембицу']\n",
      "['большая', 'колонна_отступавшая', 'по', 'шоссе', 'от', 'перемышля', 'к', 'саноку_обстреливалась', 'с', 'высот_нашей_батареей', 'и', 'бежала_бросив', 'парки_обоз', 'и', 'автомобили']\n",
      "['вылазки_гарнизона_перемышля_остаются', 'безуспешными']\n",
      "['при', 'продолжающемся_отступлении', 'австрийцев_обнаруживается', 'полное_перемешивание', 'их', 'частей_захватываются', 'новые', 'партии_пленных_орудия', 'и', 'прочая_материальная', 'часть']\n",
      "['на', 'перевале_ужок', 'мы', 'разбили_неприятельский', 'отряд', 'взяли', 'его', 'артиллерию', 'и', 'много', 'пленных', 'и', 'продолжая_преследовать_вступили', 'в', 'пределы_венгрии']\n",
      "['русский_инвалид_16_сентября', '1914_года.министерство', 'народного_просвещения', 'в', 'виду_происходящих_чрезвычайных', 'событий', 'признало_соответственным', 'в', 'день', 'годовщины_со', 'дня', 'рождения_м.ю_лермонтова_2-го', 'октября_1914', 'года', 'ограничиться_совершением', 'в', 'учебных_заведениях_панихиды', 'по', 'поэту_отложив_празднование_юбилея', 'до', 'более', 'благоприятного_времени']\n",
      "['русский_инвалид_16_сентября', '1914_года.штабс-капитан', 'п._н.', 'нестеров', 'на', 'днях_увидев', 'в', 'районе_желтиева', 'в', 'галиции_летящий_над', 'нашим_расположением', 'австрийский_аэроплан', 'собиравшийся_бросить', 'бомбы_взлетел', 'на', 'воздух_атаковал_неприятеля', 'и', 'протаранил_неприятельский', 'аппарат_предотвратив_жертвы', 'в', 'наших_войсках']\n",
      "['сам_нестеров', 'при_этом', 'погиб_смертью_героя']\n",
      "['по_словам', 'доставленных', 'в', 'киев_пленных_австрийских_офицеров', 'всей_неприятельской', 'армии', 'хорошо', 'известно', 'имя_нестерова']\n",
      "['во_время', 'воздушных_разведок_русских_авиаторов', 'австрийцы_всегда', 'безошибочно_определяли_каким_аппаратом', 'управлял_нестеров']\n",
      "['когда_показывался_аэроплан-птица_красиво', 'и', 'вольно_паривший', 'в', 'воздухе_австрийцы', 'указывали_das', 'ist_nesteroff']\n",
      "['австрийцы_боялись_покойного', 'и', 'все', 'их', 'усилия', 'были', 'направлены', 'к', 'прекращению', 'его', 'деятельности']\n",
      "['за', 'задержание_отважного_летчика', 'была', 'объявлена_большая', 'премия']\n",
      "['нестеров_погиб', 'в', '27', 'лет']\n",
      "['после', 'нестерова_остались', 'жена', 'и', 'двое', 'детей', 'девочка_5-ти', 'лет', 'и', 'мальчик_3-х', 'лет']\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scoring функция npmi захватывает больше всего н-граммов",
   "id": "9ad67370b99c88c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
